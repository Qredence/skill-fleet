# Adaptive Metric Weighting

**Date**: January 21, 2026  
**Status**: âœ… Production Ready  
**API Endpoint**: `POST /api/v1/evaluation/adaptive-weights`

## Overview

Adaptive Metric Weighting enables the skill evaluation system to prioritize different metrics based on the detected skill **style**. This ensures that evaluation metrics align with the actual purpose and design of each skill, improving quality scores and optimizer effectiveness.

## Three Skill Styles

### 1. Navigation Hub (`navigation_hub`)

**Characteristics**: 
- Clear, well-organized guide with multiple examples
- Prioritizes **clarity** and **structure**
- Emphasizes **readability** and **coverage**

**Metric Weights**:
- `skill_quality`: 0.30 (â†‘ prioritized)
- `semantic_f1`: 0.15
- `entity_f1`: 0.05 (â†“ de-emphasized)
- `readability`: 0.35 (â†‘ highest)
- `coverage`: 0.15 (â†‘ many examples expected)

**Example Skills**:
- Python Async Programming Guide
- Docker Best Practices
- React Hooks Tutorial

### 2. Comprehensive (`comprehensive`)

**Characteristics**:
- Balanced coverage with patterns and examples
- Addresses multiple use cases
- Good for general-purpose reference

**Metric Weights**:
- `skill_quality`: 0.25
- `semantic_f1`: 0.25 (balanced)
- `entity_f1`: 0.20
- `readability`: 0.20
- `coverage`: 0.10

**Example Skills**:
- Advanced Python Concurrency
- Testing Strategies
- API Design Patterns

### 3. Minimal (`minimal`)

**Characteristics**:
- Concise and correct
- Minimal overhead, maximum value
- Prioritizes semantic correctness

**Metric Weights**:
- `skill_quality`: 0.20
- `semantic_f1`: 0.50 (â†‘â†‘ highest, pure correctness)
- `entity_f1`: 0.15
- `readability`: 0.10
- `coverage`: 0.05 (â†“ few examples ok)

**Example Skills**:
- Quick Git Cheatsheet
- SQL Window Functions Reference
- Kubernetes API Quick Reference

## API Usage

### Detect Style and Get Weights

```bash
curl -X POST http://localhost:8000/api/v1/evaluation/adaptive-weights \
  -H "Content-Type: application/json" \
  -d '{
    "skill_title": "Python Async Programming",
    "skill_content": "[skill content here]",
    "skill_description": "Complete guide to asyncio framework",
    "current_scores": {
      "skill_quality": 0.8,
      "semantic_f1": 0.75,
      "readability": 0.9
    }
  }'
```

### Response

```json
{
  "style": "navigation_hub",
  "confidence": 0.92,
  "reasoning": "Skill emphasizes clarity with multiple examples...",
  "weights": {
    "skill_quality": 0.30,
    "semantic_f1": 0.15,
    "entity_f1": 0.05,
    "readability": 0.35,
    "coverage": 0.15
  },
  "composite_score": 0.82,
  "expected_improvement": "+5.2% on composite score"
}
```

## Usage in Evaluation Pipeline

### Manual Integration

```python
from skill_fleet.core.dspy.metrics.adaptive_weighting import AdaptiveMetricWeighting, compute_adaptive_score

# Initialize
weighting = AdaptiveMetricWeighting()

# Detect style
style, confidence, reasoning = weighting.detect_style(
    skill_title="My Skill",
    skill_content="[skill markdown]",
    skill_description="A skill description"
)

# Get weights for style
weights = weighting.get_weights(style)

# Compute adaptive score
scores = {
    "skill_quality": 0.8,
    "semantic_f1": 0.75,
    "readability": 0.9,
    "coverage": 0.85,
    "entity_f1": 0.7
}

result = compute_adaptive_score(scores, style)
print(f"Composite Score: {result['composite']}")
```

### Integration with Optimizer

```python
# In your skill creation workflow:
from skill_fleet.core.dspy.metrics.adaptive_weighting import AdaptiveMetricWeighting

def adaptive_metric(example, pred, trace=None):
    """Metric that adapts based on detected skill style."""
    weighting = AdaptiveMetricWeighting()
    
    # Detect style from generated skill
    style, _, _ = weighting.detect_style(
        skill_title=pred.title,
        skill_content=pred.content,
        skill_description=pred.description
    )
    
    # Get adaptive weights
    weights = weighting.get_weights(style)
    
    # Compute scores using your metrics
    scores = {
        "skill_quality": evaluate_quality(pred),
        "semantic_f1": evaluate_semantics(pred, example),
        "readability": evaluate_readability(pred),
        "coverage": evaluate_coverage(pred),
        "entity_f1": evaluate_entities(pred, example)
    }
    
    # Apply adaptive weighting
    composite = weighting.apply_weights(scores, weights)
    return composite

# Use with optimizer
optimizer = dspy.MIPROv2(metric=adaptive_metric, auto="light")
optimized = optimizer.compile(program, trainset=trainset)
```

## Implementation Details

### Core Components

**File**: `src/skill_fleet/core/dspy/metrics/adaptive_weighting.py`

1. **SkillStyle Enum**: Three style categories
2. **MetricWeights Dataclass**: Style-specific weight configuration
3. **DetectSkillStyle Signature**: DSPy signature for style detection
4. **SkillStyleDetector Module**: LLM-powered style detection
5. **AdaptiveMetricWeighting Class**: Main interface

### Testing

26 comprehensive tests covering:
- âœ… MetricWeights class (7 tests)
- âœ… SkillStyleDetector module (4 tests)
- âœ… AdaptiveMetricWeighting class (8 tests)
- âœ… compute_adaptive_score function (5 tests)
- âœ… Full workflow integration (2 tests)

All tests passing âœ…

## Performance

- **Style Detection**: ~1-2 seconds (LLM call)
- **Weight Computation**: <1ms
- **Adaptive Scoring**: O(n) where n=5 metrics
- **Overall**: Negligible overhead

## Best Practices

### 1. Use with Optimization

Adaptive weighting is most effective when combined with DSPy optimizers:

```python
# MIPROv2 with adaptive metric
optimizer = dspy.MIPROv2(
    metric=adaptive_metric,
    auto="light"
)

# GEPA with adaptive metric
gepa = dspy.GEPA(
    metric=adaptive_metric,
    num_candidates=5
)
```

### 2. Confidence Scores

Always check the confidence score from style detection:

```python
style, confidence, reasoning = weighting.detect_style(...)

if confidence < 0.7:
    # Low confidence - consider manual review
    print(f"Style detection confidence: {confidence}")
    print(f"Reasoning: {reasoning}")
```

### 3. Cache Detected Styles

For repeated evaluations of the same skill, cache the detected style:

```python
# Bad: Re-detects style every time
for example in trainset:
    style, _, _ = weighting.detect_style(...)

# Good: Cache the detection
styles_cache = {}
for example in trainset:
    if example.skill_id not in styles_cache:
        style, _, _ = weighting.detect_style(
            skill_title=example.title,
            skill_content=example.content,
            skill_description=example.description
        )
        styles_cache[example.skill_id] = style
    else:
        style = styles_cache[example.skill_id]
```

### 4. Understand Weight Normalization

Weights are automatically normalized (sum = 1.0):

```python
# These are equivalent:
result1 = weighting.apply_weights(scores, {
    "skill_quality": 0.30,
    "semantic_f1": 0.15,
    "entity_f1": 0.05,
    "readability": 0.35,
    "coverage": 0.15
})

result2 = weighting.apply_weights(scores, {
    "skill_quality": 0.60,
    "semantic_f1": 0.30,
    "entity_f1": 0.10,
    "readability": 0.70,
    "coverage": 0.30
})  # Normalized automatically

assert abs(result1 - result2) < 0.001
```

## Limitations & Future Improvements

### Current Limitations

1. **LLM-Dependent**: Style detection accuracy depends on LLM quality
2. **No Caching**: Repeated calls re-detect style (can optimize in Phase 1.2)
3. **3 Styles Only**: Additional styles (pattern-focused, example-heavy) could be added in Phase 2

### Planned Improvements (Phase 1.2+)

- [ ] Implement style detection caching
- [ ] Module-level DSPy component caching
- [ ] Optimize signatures with MIPROv2
- [ ] Support additional style categories
- [ ] Track style detection accuracy metrics

## Troubleshooting

### Low Confidence Detection

**Problem**: Skill style detected with confidence < 0.7

**Solution**:
1. Ensure skill has clear frontmatter with `name` and `description`
2. Include "When to Use" or style-indicating sections
3. Review the reasoning output to understand what confused detection

### Unexpected Weight Application

**Problem**: Adaptive weights don't match expected style

**Solution**:
1. Verify style was correctly detected (check confidence score)
2. Print the `reasoning` to understand why that style was chosen
3. Consider manual style assignment if detection is unreliable

### Performance Impact

**Problem**: Style detection adds latency

**Solution**:
1. Cache detected styles in evaluation pipeline
2. Run style detection asynchronously if possible
3. Use confidence threshold to skip detection for clear cases

## References

- **Implementation**: [adaptive_weighting.py](../src/skill_fleet/core/dspy/metrics/adaptive_weighting.py)
- **Tests**: [test_adaptive_weighting.py](../tests/unit/test_adaptive_weighting.py)
- **API Endpoint**: [evaluation.py routes](../src/skill_fleet/api/routes/evaluation.py#L382)
- **Review Document**: [IMPLEMENTATION_REVIEW_1_1.md](../IMPLEMENTATION_REVIEW_1_1.md)

## Integration Timeline

| Phase | Task | Status |
|-------|------|--------|
| **1.1** | Core implementation + API endpoint | âœ… Complete |
| **1.2** | Integrate with Phase 3 validation | ðŸŸ¡ Planned |
| **1.4** | E2E workflow testing | ðŸŸ¡ Planned |
| **2.0** | Production deployment | ðŸŸ¡ Planned |

---

**Last Updated**: January 21, 2026  
**Status**: Production Ready âœ…
