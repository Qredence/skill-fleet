"""Job management for async skill creation and HITL tracking."""

from __future__ import annotations

import asyncio
import json
import logging
import string
import time
import uuid
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

from skill_fleet.common.logging_utils import sanitize_for_log

from ...common.security import resolve_path_within_root
from ..schemas.models import DeepUnderstandingState, JobState, TDDWorkflowState

logger = logging.getLogger(__name__)


def _is_safe_job_id(job_id: str) -> bool:
    """
    Return True if the job_id is safe to use as a filename component.

    This restricts job IDs to a conservative character set to prevent
    path traversal or injection when constructing paths like
    SESSION_DIR / f"{job_id}.json".
    """
    if not job_id:
        return False
    # Allow only ASCII letters, digits, dash and underscore.
    # UUIDs generated by create_job() fit within this set.
    allowed_chars = set(string.ascii_letters + string.digits + "-_")
    return all(ch in allowed_chars for ch in job_id)


def _canonicalize_session_job_id(job_id: str) -> str | None:
    """
    Canonicalize and validate job IDs used for session file paths.

    UUID-shaped IDs are normalized to lowercase canonical form. Non-UUID IDs are
    accepted only if they pass the strict allowlist in `_is_safe_job_id`.
    """
    if not job_id:
        return None

    try:
        return str(uuid.UUID(job_id))
    except (ValueError, AttributeError, TypeError):
        if _is_safe_job_id(job_id):
            return job_id
        return None


def _resolve_session_file_path(canonical_job_id: str) -> Path:
    """
    Resolve a session file path from a canonicalized job ID.

    Raises:
        ValueError: If the path escapes the root directory.

    """
    return resolve_path_within_root(SESSION_DIR, f"{canonical_job_id}.json")


# In-memory job store with TTL eviction (use Redis in production)
class JobStore:
    """
    Job store with automatic TTL eviction.

    Prevents memory leaks by evicting old jobs after MAX_AGE_HOURS.
    Designed for asyncio context (single-threaded event loop).
    """

    MAX_JOBS = 1000  # Maximum number of jobs to keep in memory
    MAX_AGE_HOURS = 24  # Evict jobs older than this

    def __init__(self):
        self._jobs: dict[str, JobState] = {}
        self._access_times: dict[str, float] = {}

    def _evict_if_needed(self):
        """Evict old jobs if we're over capacity or jobs are too old."""
        now = time.time()
        max_age_seconds = self.MAX_AGE_HOURS * 3600

        # Evict by age
        expired = [
            job_id
            for job_id, last_access in self._access_times.items()
            if now - last_access > max_age_seconds
        ]
        for job_id in expired:
            self._evict_job(job_id)

        # Evict by count (LRU - evict least recently accessed)
        while len(self._jobs) > self.MAX_JOBS and self._access_times:
            # Find oldest access
            oldest_job = min(self._access_times, key=lambda job_id: self._access_times[job_id])
            self._evict_job(oldest_job)

    def _evict_job(self, job_id: str):
        """Remove a job from memory."""
        self._jobs.pop(job_id, None)
        self._access_times.pop(job_id, None)
        logger.debug(f"Evicted job {job_id} from memory")

    def __getitem__(self, job_id: str) -> JobState:
        """Get job and update access time."""
        self._access_times[job_id] = time.time()
        return self._jobs[job_id]

    def __setitem__(self, job_id: str, job: JobState):
        """Set job and update access time."""
        self._evict_if_needed()
        self._jobs[job_id] = job
        self._access_times[job_id] = time.time()

    def __contains__(self, job_id: str) -> bool:
        """Check if job exists."""
        return job_id in self._jobs

    def get(self, job_id: str, default=None):
        """Get job with default."""
        if job_id in self._jobs:
            self._access_times[job_id] = time.time()
            return self._jobs[job_id]
        return default

    def pop(self, job_id: str, default=None):
        """Remove and return job."""
        self._access_times.pop(job_id, None)
        return self._jobs.pop(job_id, default)

    def keys(self):
        """Return job IDs."""
        return self._jobs.keys()

    def items(self):
        """Return job items."""
        return self._jobs.items()

    def values(self):
        """Return job values."""
        return self._jobs.values()

    def __iter__(self):
        """Iterate over job IDs."""
        return iter(self._jobs)

    def __len__(self) -> int:
        """Return number of jobs."""
        return len(self._jobs)


JOBS: JobStore = JobStore()


async def create_job(task_description: str, user_id: str | None = None) -> str:
    """Create a new job and return its ID."""
    from .job_manager import get_job_manager

    job_id = str(uuid.uuid4())
    job_state = JobState(
        job_id=job_id,
        status="pending",
        task_description=task_description,
        user_id=user_id or "default",
    )

    # Register in both JOBS dict (for backward compat) and JobManager
    JOBS[job_id] = job_state
    manager = get_job_manager()
    await manager.create_job(job_state)

    return job_id


async def get_job(job_id: str) -> JobState | None:
    """Retrieve a job by its ID, loading from disk if necessary."""
    job = JOBS.get(job_id)
    if job:
        return job

    # Fallback to JobManager for persistence
    from .job_manager import get_job_manager

    manager = get_job_manager()
    return await manager.get_job(job_id)


async def update_job(job_id: str, updates: dict[str, Any]) -> JobState | None:
    """
    Update a job with the provided updates.

    Args:
        job_id: Job identifier
        updates: Dictionary of fields to update

    Returns:
        Updated JobState if found, None otherwise

    """
    from .job_manager import get_job_manager

    manager = get_job_manager()
    updated_job = await manager.update_job(job_id, updates)
    if updated_job:
        JOBS[job_id] = updated_job
    return updated_job


async def wait_for_hitl_response(job_id: str, timeout: float = 3600.0) -> dict[str, Any]:
    """
    Wait for user to provide HITL response via API.

    This uses an event-driven mechanism instead of polling to avoid:
    - unnecessary latency (poll interval)
    - duplicate prompts in clients that poll immediately after POSTing a response

    Uses asyncio.Event on the JobState instance for atomic signaling.
    """
    job = JOBS.get(job_id)
    if job is None:
        # Fall back to JobManager-backed lookup (e.g., tests that only use JobManager).
        job = await get_job(job_id)
        if job is None:
            raise KeyError(job_id)
        JOBS[job_id] = job

    # Ensure event is initialized (handles loaded sessions)
    if job.hitl_event is None:
        job.hitl_event = asyncio.Event()

    # Fast-path: response already present (race-safe check)
    response = job.hitl_response
    if response is not None:
        job.hitl_response = None
        job.hitl_event.clear()  # Reset event for next interaction
        job.updated_at = datetime.now(UTC)
        return response

    # Wait for response to be set
    try:
        await asyncio.wait_for(job.hitl_event.wait(), timeout=timeout)
    except TimeoutError as exc:
        raise TimeoutError("HITL response timed out") from exc

    # Atomic check and clear - response is guaranteed to be set after event fires
    response = job.hitl_response
    assert response is not None, "Response should be set after event fires"  # noqa: S101
    job.hitl_response = None
    job.hitl_event.clear()  # Reset for next interaction
    job.updated_at = datetime.now(UTC)
    return response


async def notify_hitl_response(job_id: str, response: dict[str, Any]) -> None:
    """
    Notify the in-flight HITL waiter that a response arrived.

    This is race-safe: the response is stored before setting the event,
    ensuring that any waiters will see the new response.
    """
    # Prefer the in-memory JOBS object for signaling, since wait_for_hitl_response()
    # blocks on JOBS[job_id].hitl_event. JobManager may return a distinct instance
    # in some persistence paths, so we ensure both are updated.
    job = JOBS.get(job_id)

    if job is not None:
        if job.hitl_event is None:
            job.hitl_event = asyncio.Event()
        job.hitl_response = response
        job.hitl_event.set()
        job.updated_at = datetime.now(UTC)

    from .job_manager import get_job_manager

    manager = get_job_manager()
    job_manager_job = await manager.get_job(job_id)
    if job_manager_job is None:
        return

    if job_manager_job.hitl_event is None:
        job_manager_job.hitl_event = asyncio.Event()
    job_manager_job.hitl_response = response
    job_manager_job.hitl_event.set()
    job_manager_job.updated_at = datetime.now(UTC)

    # Keep JOBS in sync even if it didn't exist (e.g., loaded from DB-only paths).
    JOBS[job_id] = job_manager_job

    await manager.update_job(
        job_id,
        {"updated_at": job_manager_job.updated_at, "hitl_response": response},
    )


# =============================================================================
# Session Persistence
# =============================================================================


# Session directory for persistence
SESSION_DIR = Path(".skill_fleet_sessions")
SESSION_DIR.mkdir(exist_ok=True)


def save_job_session(job_id: str) -> bool:
    """
    Save job state to disk for persistence.

    Args:
        job_id: The job ID to save

    Returns:
        True if save succeeded, False otherwise

    """
    canonical_job_id = _canonicalize_session_job_id(job_id)
    if canonical_job_id is None:
        logger.warning("Cannot save session: unsafe job id %s", sanitize_for_log(job_id))
        return False

    job = JOBS.get(canonical_job_id) or JOBS.get(job_id)
    if not job:
        logger.warning("Cannot save session: job %s not found", sanitize_for_log(job_id))
        return False

    try:
        session_file = _resolve_session_file_path(canonical_job_id)
        session_file = _resolve_session_file_path(canonical_job_id)
        session_data = job.model_dump(mode="json", exclude_none=True)
        session_data["job_id"] = canonical_job_id
        session_file.write_text(json.dumps(session_data, indent=2, default=str), encoding="utf-8")
        logger.debug("Saved session for job %s", sanitize_for_log(job_id))
        return True
    except ValueError as e:
        logger.warning(
            "Cannot save session for job %s: %s",
            sanitize_for_log(job_id),
            sanitize_for_log(e),
        )
        return False
    except Exception as e:
        logger.error(
            "Failed to save session for job %s: %s",
            sanitize_for_log(job_id),
            sanitize_for_log(e),
        )
        return False


async def save_job_session_async(job_id: str) -> bool:
    """
    Save job state to disk for persistence (async version).

    This function runs the blocking file I/O in a thread pool to avoid
    blocking the event loop.

    Args:
        job_id: The job ID to save

    Returns:
        True if save succeeded, False otherwise

    """
    canonical_job_id = _canonicalize_session_job_id(job_id)
    if canonical_job_id is None:
        logger.warning("Cannot save session: unsafe job id %s", sanitize_for_log(job_id))
        return False

    job = JOBS.get(canonical_job_id) or JOBS.get(job_id)
    if not job:
        logger.warning("Cannot save session: job %s not found", sanitize_for_log(job_id))
        return False

    try:
        session_file = _resolve_session_file_path(canonical_job_id)
        session_data = job.model_dump(mode="json", exclude_none=True)
        session_data["job_id"] = canonical_job_id
        json_str = json.dumps(session_data, indent=2, default=str)
        # Run blocking I/O in thread pool
        await asyncio.to_thread(session_file.write_text, json_str, encoding="utf-8")
        logger.debug("Saved session for job %s", sanitize_for_log(job_id))
        return True
    except ValueError as e:
        logger.warning(
            "Cannot save session for job %s: %s",
            sanitize_for_log(job_id),
            sanitize_for_log(e),
        )
        return False
    except Exception as e:
        logger.error(
            "Failed to save session for job %s: %s",
            sanitize_for_log(job_id),
            sanitize_for_log(e),
        )
        return False


def load_job_session(job_id: str) -> JobState | None:
    """
    Load job state from disk.

    Args:
        job_id: The job ID to load

    Returns:
        JobState if found, None otherwise

    """
    canonical_job_id = _canonicalize_session_job_id(job_id)
    if canonical_job_id is None:
        logger.warning("Cannot load session: unsafe job id %s", sanitize_for_log(job_id))
        return None

    try:
        session_file = _resolve_session_file_path(canonical_job_id)
        if not session_file.exists():
            return None

        session_data = json.loads(session_file.read_text(encoding="utf-8"))

        # Reconstruct JobState from saved data
        # Need to handle nested models properly
        tdd_data = session_data.pop("tdd_workflow", {})
        deep_data = session_data.pop("deep_understanding", {})

        job = JobState(**session_data)

        # Restore nested models
        if tdd_data:
            job.tdd_workflow = TDDWorkflowState(**tdd_data)
        if deep_data:
            job.deep_understanding = DeepUnderstandingState(**deep_data)

        # Reinitialize hitl_event and hitl_lock (not serialized)
        job.hitl_event = asyncio.Event()
        job.hitl_lock = asyncio.Lock()

        JOBS[job.job_id] = job
        logger.info("Loaded session for job %s", sanitize_for_log(job_id))
        return job

    except ValueError as e:
        logger.warning(
            "Cannot load session for job %s: %s",
            sanitize_for_log(job_id),
            sanitize_for_log(e),
        )
        return None

    except Exception as e:
        logger.error(
            "Failed to load session for job %s: %s",
            sanitize_for_log(job_id),
            sanitize_for_log(e),
        )
        return None


def list_saved_sessions() -> list[str]:
    """
    List all saved session IDs.

    Returns:
        List of job IDs with saved sessions

    """
    try:
        return [f.stem for f in SESSION_DIR.glob("*.json")]
    except Exception:
        return []


def delete_job_session(job_id: str) -> bool:
    """
    Delete a saved session.

    Args:
        job_id: The job ID to delete

    Returns:
        True if deletion succeeded, False otherwise

    """
    canonical_job_id = _canonicalize_session_job_id(job_id)
    if canonical_job_id is None:
        logger.warning("Cannot delete session: unsafe job id %s", sanitize_for_log(job_id))
        return False

    try:
        session_file = _resolve_session_file_path(canonical_job_id)
        if session_file.exists():
            session_file.unlink()
            return True
        return False
    except ValueError as e:
        logger.warning(
            "Cannot delete session for job %s: %s",
            sanitize_for_log(job_id),
            sanitize_for_log(e),
        )
        return False
    except Exception as e:
        logger.error(
            "Failed to delete session for job %s: %s",
            sanitize_for_log(job_id),
            sanitize_for_log(e),
        )
        return False


def cleanup_old_sessions(max_age_hours: float = 24.0) -> int:
    """
    Clean up old session files and evict from memory.

    Args:
        max_age_hours: Maximum age in hours before deletion

    Returns:
        Number of sessions cleaned up

    """
    cleaned = 0
    cutoff_time = time.time() - (max_age_hours * 3600)

    # Clean up files
    try:
        for session_file in SESSION_DIR.glob("*.json"):
            if session_file.stat().st_mtime < cutoff_time:
                try:
                    session_file.unlink()
                    cleaned += 1
                except Exception as exc:
                    # Ignore errors when deleting individual files, but log for debugging.
                    logger.debug("Failed to delete session file %s: %s", session_file, exc)
    except Exception as exc:
        # Ignore errors during cleanup (e.g., directory doesn't exist), but log for debugging.
        logger.debug("Failed to cleanup session files: %s", exc)

    # Also trigger memory eviction
    JOBS._evict_if_needed()

    if cleaned > 0:
        logger.info(f"Cleaned up {cleaned} old session(s)")

    return cleaned
