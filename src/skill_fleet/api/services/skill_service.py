"""
FastAPI service layer for skill creation operations.

This service bridges FastAPI routes to the skill creation workflow orchestrators.
It provides a clean API for skill creation, validation, and refinement operations.

The service layer handles:
- Creating skills from natural language descriptions
- Managing skill creation jobs (async background tasks)
- Saving skills to draft area
- Retrieving skill metadata
- Validating and refining skills
- Hierarchical MLflow tracking with parent runs and child runs for each phase

This service uses the workflows layer orchestrators for clean separation of concerns.
"""

from __future__ import annotations

import logging
import uuid
from typing import TYPE_CHECKING, Any

from skill_fleet.core.workflows.skill_creation.generation import GenerationWorkflow
from skill_fleet.core.workflows.skill_creation.understanding import UnderstandingWorkflow
from skill_fleet.core.workflows.skill_creation.validation import ValidationWorkflow
from skill_fleet.infrastructure.tracing.mlflow import (
    end_parent_run,
    log_quality_metrics,
    log_skill_artifacts,
    log_tags,
    log_validation_results,
    start_child_run,
    start_parent_run,
)
from skill_fleet.taxonomy.manager import TaxonomyManager

from ...core.models import SkillCreationResult
from ..schemas.models import JobState
from .job_manager import get_job_manager

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from pathlib import Path

    from ..schemas.skills import CreateSkillRequest


class SkillService:
    """Service for managing skill creation operations."""

    def __init__(
        self,
        skills_root: Path,
        drafts_root: Path,
    ):
        """
        Initialize skill service.

        Args:
            skills_root: Root directory for skills taxonomy
            drafts_root: Root directory for draft skills

        """
        self.skills_root = skills_root
        self.drafts_root = drafts_root
        self.taxonomy_manager = TaxonomyManager(skills_root)
        self.understanding_workflow = UnderstandingWorkflow()
        self.generation_workflow = GenerationWorkflow()
        self.validation_workflow = ValidationWorkflow()

    async def create_skill(
        self,
        request: CreateSkillRequest,
        existing_job_id: str | None = None,
        hitl_callback: Any | None = None,
        progress_callback: Any | None = None,
        enable_mlflow: bool = True,
    ) -> SkillCreationResult:
        """
        Create a new skill from a natural language description.

        Uses the workflows layer orchestrators for clean separation of concerns:
        1. TaskAnalysisOrchestrator - Phase 1: Understanding & Planning
        2. ContentGenerationOrchestrator - Phase 2: Content Generation
        3. QualityAssuranceOrchestrator - Phase 3: Validation & Refinement

        This method creates a hierarchical MLflow structure with:
        - Parent run for the entire skill creation workflow
        - Child runs for each phase (task analysis, content generation, quality assurance)
        - Complete artifacts logged at the parent level

        Args:
            request: Skill creation request with task description and user ID
            existing_job_id: Optional ID of existing job to resume
            hitl_callback: Optional callback for HITL interactions
            progress_callback: Optional callback for progress updates
            enable_mlflow: Whether to track with MLflow (default: True)

        Returns:
            SkillCreationResult: Result of the skill creation workflow

        """
        job_manager = get_job_manager()

        if existing_job_id:
            job_id = existing_job_id
            job = job_manager.get_job(job_id)
            if not job:
                # Should not happen in resume flow
                job_id = existing_job_id
        else:
            job_id = str(uuid.uuid4())
            # Create job state
            job_state = JobState(
                job_id=job_id,
                task_description=request.task_description,
                user_id=request.user_id,
                status="running",
            )
            job_manager.create_job(job_state)

        logger.info(
            "Creating skill for user %s: %s (Job ID: %s)",
            request.user_id,
            request.task_description[:100],
            job_id,
        )

        # Check for previous answers to inject into context
        previous_answers = {}
        job = job_manager.get_job(job_id)
        if job and job.deep_understanding.answers:
            previous_answers = {"prior_clarifications": job.deep_understanding.answers}

        # Provide real taxonomy context
        taxonomy_structure = self.taxonomy_manager.get_relevant_branches(request.task_description)
        mounted_skills = self.taxonomy_manager.get_mounted_skills(request.user_id)

        # Use workflows initialized in __init__
        understanding_workflow = self.understanding_workflow
        generation_workflow = self.generation_workflow
        validation_workflow = self.validation_workflow

        # Start MLflow parent run for hierarchical tracking
        parent_run_id = None
        if enable_mlflow:
            # Extract skill type from request if available, otherwise None
            skill_type = getattr(request, "skill_type", None)
            parent_run_id = start_parent_run(
                run_name=request.task_description[:100],
                user_id=request.user_id,
                job_id=job_id,
                skill_type=skill_type,
                description=f"Skill creation: {request.task_description[:200]}",
            )

        try:
            # Phase 1: Task Analysis & Planning
            if progress_callback:
                progress_callback("phase1", "Analyzing requirements and planning skill structure")

            # Build user context dict
            user_context = {"user_id": request.user_id, **previous_answers}

            if enable_mlflow:
                # Use child run for phase 1
                with start_child_run("phase1_task_analysis"):
                    phase1_result = await understanding_workflow.execute(
                        task_description=request.task_description,
                        user_context=user_context,
                        taxonomy_structure=taxonomy_structure,
                        existing_skills=mounted_skills,
                    )
            else:
                phase1_result = await understanding_workflow.execute(
                    task_description=request.task_description,
                    user_context=user_context,
                    taxonomy_structure=taxonomy_structure,
                    existing_skills=mounted_skills,
                )

            # Check for HITL checkpoint
            if phase1_result.get("status") == "pending_user_input":
                # Suspend job for HITL
                job = job_manager.get_job(job_id)
                if job:
                    job.status = "pending_user_input"
                    job.hitl_data = phase1_result.get("hitl_data", {})
                    job.hitl_type = phase1_result.get("hitl_type", "clarify")
                    job_manager.update_job(
                        job_id,
                        {
                            "status": "pending_user_input",
                            "hitl_data": job.hitl_data,
                            "hitl_type": job.hitl_type,
                        },
                    )

                return SkillCreationResult(
                    job_id=job_id,
                    status="pending_user_input",
                    message="Clarification needed from user",
                    hitl_context=phase1_result.get("hitl_data", {}),
                )

            # Phase 2: Content Generation
            if progress_callback:
                progress_callback("phase2", "Generating skill content")

            if enable_mlflow:
                with start_child_run("phase2_content_generation"):
                    phase2_result = await generation_workflow.execute(
                        plan=phase1_result.get("plan", {}),
                        understanding=phase1_result,
                        enable_hitl_preview=False,
                    )
            else:
                phase2_result = await generation_workflow.execute(
                    plan=phase1_result.get("plan", {}),
                    understanding=phase1_result,
                    enable_hitl_preview=False,
                )

            # Phase 3: Quality Assurance
            if progress_callback:
                progress_callback("phase3", "Validating and refining skill")

            # Get taxonomy path from phase 1 result
            taxonomy_path = phase1_result.get("plan", {}).get("taxonomy_path", "")

            if enable_mlflow:
                with start_child_run("phase3_quality_assurance"):
                    phase3_result = await validation_workflow.execute(
                        skill_content=phase2_result.get("skill_content", ""),
                        plan=phase1_result.get("plan", {}),
                        taxonomy_path=taxonomy_path,
                        enable_hitl_review=False,
                    )
            else:
                phase3_result = await validation_workflow.execute(
                    skill_content=phase2_result.get("skill_content", ""),
                    plan=phase1_result.get("plan", {}),
                    taxonomy_path=taxonomy_path,
                    enable_hitl_review=False,
                )

            # Construct result from workflow outputs
            result = SkillCreationResult(
                status="completed"
                if phase3_result.get("validation_report", {}).get("passed", False)
                else "pending_review",
                skill_content=phase2_result.get("skill_content"),
                metadata=phase1_result.get("plan", {}).get("skill_metadata"),
                validation_report=phase3_result.get("validation_report"),
                quality_assessment=phase3_result.get("quality_assessment"),
            )

            # Log final artifacts at parent level
            if enable_mlflow and parent_run_id:
                skill_metadata = phase1_result.get("plan", {}).get("skill_metadata", {})

                # Log skill type tag if available
                if skill_metadata.get("type"):
                    log_tags({"skill_type": skill_metadata["type"]})

                # Log validation results as metrics and artifact
                validation_report = phase3_result.get("validation_report", {})
                if validation_report:
                    log_validation_results(validation_report)

                # Log quality assessment metrics
                quality_assessment = phase3_result.get("quality_assessment", {})
                if quality_assessment:
                    log_quality_metrics(quality_assessment)

                # Log complete skill artifacts
                log_skill_artifacts(
                    content=phase2_result.get("skill_content"),
                    metadata=skill_metadata,
                    validation_report=validation_report,
                    quality_assessment=quality_assessment,
                )

            return result

        finally:
            # End parent run
            if enable_mlflow and parent_run_id:
                end_parent_run()

    async def resume_skill_creation(
        self, job_id: str, answers: dict[str, Any]
    ) -> SkillCreationResult:
        """
        Resume skill creation with HITL answers.

        Args:
            job_id: ID of the suspended job
            answers: User answers to clarification questions

        Returns:
            SkillCreationResult

        """
        job_manager = get_job_manager()
        job = job_manager.get_job(job_id)
        if not job:
            raise ValueError(f"Job {job_id} not found")

        # Update answers in DeepUnderstandingState (append as record)
        job.deep_understanding.answers.append(answers)

        job.status = "running"
        job_manager.update_job(
            job_id, {"status": "running", "deep_understanding": job.deep_understanding}
        )

        # Re-construct request from stored job state
        from ..schemas.skills import CreateSkillRequest

        request = CreateSkillRequest(task_description=job.task_description, user_id=job.user_id)

        return await self.create_skill(request, existing_job_id=job_id)

    def save_skill_to_draft(
        self,
        job_id: str,
        result: SkillCreationResult,
    ) -> str | None:
        """
        Save a completed skill to the draft area.

        Args:
            job_id: Unique job identifier
            result: SkillCreationResult from workflow

        Returns:
            Path where draft skill was saved, or None if save failed

        """
        from ..utils.draft_save import save_skill_to_draft

        return save_skill_to_draft(
            drafts_root=self.drafts_root,
            job_id=job_id,
            result=result,
        )

    def get_skill_by_path(self, path: str) -> dict[str, Any]:
        """
        Get details for a skill by its path or ID.

        Args:
            path: Skill path or ID

        Returns:
            Dictionary with skill details

        Raises:
            FileNotFoundError: If skill not found

        """
        # Resolve location (handles aliases + legacy paths via manager)
        canonical_path = self.taxonomy_manager.resolve_skill_location(path)

        # Load metadata
        meta = self.taxonomy_manager.get_skill_metadata(canonical_path)
        if not meta:
            meta = self.taxonomy_manager._try_load_skill_by_id(canonical_path)

        if not meta:
            raise FileNotFoundError(f"Skill not found: {path}")

        # Load content
        content = None
        if meta.path.name == "metadata.json":
            md_path = meta.path.parent / "SKILL.md"
            if md_path.exists():
                content = md_path.read_text(encoding="utf-8")

        # Convert dataclass to dict for response
        return {
            "skill_id": meta.skill_id,
            "name": meta.name,
            "description": meta.description,
            "version": meta.version,
            "type": meta.type,
            "metadata": {
                "weight": meta.weight,
                "load_priority": meta.load_priority,
                "dependencies": meta.dependencies,
                "capabilities": meta.capabilities,
                "always_loaded": meta.always_loaded,
            },
            "content": content,
        }
