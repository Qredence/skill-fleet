"""Job management for async skill creation and HITL tracking."""

from __future__ import annotations

import asyncio
import json
import logging
import time
import uuid
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

from ...common.security import resolve_path_within_root
from ..schemas.models import DeepUnderstandingState, JobState, TDDWorkflowState

logger = logging.getLogger(__name__)


def _sanitize_for_log(value: Any) -> str:
    """Remove newline characters from values before logging to prevent log injection."""
    text = str(value)
    return text.replace("\r", "").replace("\n", "")


def _is_safe_job_id(job_id: str) -> bool:
    """
    Return True if the job_id is safe to use as a filename component.

    This restricts job IDs to a conservative character set to prevent
    path traversal or injection when constructing paths like
    SESSION_DIR / f"{job_id}.json".
    """
    if not job_id:
        return False
    # Allow only ASCII letters, digits, dash and underscore.
    # UUIDs generated by create_job() fit within this set.
    return all(ch.isalnum() or ch in "-_" for ch in job_id)


# In-memory job store with TTL eviction (use Redis in production)
class JobStore:
    """
    Job store with automatic TTL eviction.

    Prevents memory leaks by evicting old jobs after MAX_AGE_HOURS.
    Designed for asyncio context (single-threaded event loop).
    """

    MAX_JOBS = 1000  # Maximum number of jobs to keep in memory
    MAX_AGE_HOURS = 24  # Evict jobs older than this

    def __init__(self):
        self._jobs: dict[str, JobState] = {}
        self._access_times: dict[str, float] = {}

    def _evict_if_needed(self):
        """Evict old jobs if we're over capacity or jobs are too old."""
        now = time.time()
        max_age_seconds = self.MAX_AGE_HOURS * 3600

        # Evict by age
        expired = [
            job_id
            for job_id, last_access in self._access_times.items()
            if now - last_access > max_age_seconds
        ]
        for job_id in expired:
            self._evict_job(job_id)

        # Evict by count (LRU - evict least recently accessed)
        while len(self._jobs) > self.MAX_JOBS and self._access_times:
            # Find oldest access
            oldest_job = min(self._access_times, key=self._access_times.get)
            self._evict_job(oldest_job)

    def _evict_job(self, job_id: str):
        """Remove a job from memory."""
        self._jobs.pop(job_id, None)
        self._access_times.pop(job_id, None)
        logger.debug(f"Evicted job {job_id} from memory")

    def __getitem__(self, job_id: str) -> JobState:
        """Get job and update access time."""
        self._access_times[job_id] = time.time()
        return self._jobs[job_id]

    def __setitem__(self, job_id: str, job: JobState):
        """Set job and update access time."""
        self._evict_if_needed()
        self._jobs[job_id] = job
        self._access_times[job_id] = time.time()

    def __contains__(self, job_id: str) -> bool:
        """Check if job exists."""
        return job_id in self._jobs

    def get(self, job_id: str, default=None):
        """Get job with default."""
        if job_id in self._jobs:
            self._access_times[job_id] = time.time()
            return self._jobs[job_id]
        return default

    def pop(self, job_id: str, default=None):
        """Remove and return job."""
        self._access_times.pop(job_id, None)
        return self._jobs.pop(job_id, default)

    def keys(self):
        """Return job IDs."""
        return self._jobs.keys()

    def items(self):
        """Return job items."""
        return self._jobs.items()

    def values(self):
        """Return job values."""
        return self._jobs.values()

    def __iter__(self):
        """Iterate over job IDs."""
        return iter(self._jobs)

    def __len__(self) -> int:
        """Return number of jobs."""
        return len(self._jobs)


JOBS: JobStore = JobStore()


def create_job(task_description: str = "", user_id: str = "default") -> str:
    """
    Create a new job and return its unique ID.

    Args:
        task_description: The task description for this job
        user_id: The user ID creating this job

    Returns:
        The unique job ID

    """
    from .job_manager import get_job_manager

    job_id = str(uuid.uuid4())
    job_state = JobState(
        job_id=job_id,
        task_description=task_description,
        user_id=user_id,
    )

    # Register in both JOBS dict (for backward compat) and JobManager
    JOBS[job_id] = job_state
    manager = get_job_manager()
    manager.create_job(job_state)

    return job_id


def get_job(job_id: str) -> JobState | None:
    """Retrieve a job by its ID, loading from disk if necessary."""
    job = JOBS.get(job_id)
    if job:
        return job

    # Fallback to disk loading for persistence
    return load_job_session(job_id)


async def wait_for_hitl_response(job_id: str, timeout: float = 3600.0) -> dict[str, Any]:
    """
    Wait for user to provide HITL response via API.

    This uses an event-driven mechanism instead of polling to avoid:
    - unnecessary latency (poll interval)
    - duplicate prompts in clients that poll immediately after POSTing a response

    Uses asyncio.Event on the JobState instance for atomic signaling.
    """
    job = JOBS[job_id]

    # Ensure event is initialized (handles loaded sessions)
    if job.hitl_event is None:
        job.hitl_event = asyncio.Event()

    # Fast-path: response already present (race-safe check)
    response = job.hitl_response
    if response is not None:
        job.hitl_response = None
        job.hitl_event.clear()  # Reset event for next interaction
        job.updated_at = datetime.now(UTC)
        return response

    # Wait for response to be set
    try:
        await asyncio.wait_for(job.hitl_event.wait(), timeout=timeout)
    except TimeoutError as exc:
        raise TimeoutError("HITL response timed out") from exc

    # Atomic check and clear - response is guaranteed to be set after event fires
    response = job.hitl_response
    assert response is not None, "Response should be set after event fires"  # noqa: S101
    job.hitl_response = None
    job.hitl_event.clear()  # Reset for next interaction
    job.updated_at = datetime.now(UTC)
    return response


def notify_hitl_response(job_id: str, response: dict[str, Any]) -> None:
    """
    Notify the in-flight HITL waiter that a response arrived.

    This is race-safe: the event is set before storing the response,
    ensuring that any waiters will see the new response.
    """
    from .job_manager import get_job_manager

    manager = get_job_manager()
    job = manager.get_job(job_id)
    if job is None:
        return

    # Ensure event is initialized
    if job.hitl_event is None:
        job.hitl_event = asyncio.Event()

    # Store response first (atomic with event set)
    job.hitl_response = response
    job.hitl_event.set()  # Notify any waiting coroutines
    job.updated_at = datetime.now(UTC)

    # Update both memory and database
    manager.update_job(job_id, {"updated_at": job.updated_at, "hitl_response": response})


# =============================================================================
# Session Persistence
# =============================================================================


# Session directory for persistence
SESSION_DIR = Path(".skill_fleet_sessions")
SESSION_DIR.mkdir(exist_ok=True)


def save_job_session(job_id: str) -> bool:
    """
    Save job state to disk for persistence.

    Args:
        job_id: The job ID to save

    Returns:
        True if save succeeded, False otherwise

    """
    if not _is_safe_job_id(job_id):
        logger.warning("Cannot save session: unsafe job id %s", _sanitize_for_log(job_id))
        return False

    job = JOBS.get(job_id)
    if not job:
        logger.warning("Cannot save session: job %s not found", _sanitize_for_log(job_id))
        return False

    try:
        session_file = resolve_path_within_root(SESSION_DIR, f"{job_id}.json")
        session_data = job.model_dump(mode="json", exclude_none=True)
        session_file.write_text(json.dumps(session_data, indent=2, default=str), encoding="utf-8")
        logger.debug("Saved session for job %s", _sanitize_for_log(job_id))
        return True
    except ValueError as e:
        logger.warning(
            "Cannot save session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return False
    except Exception as e:
        logger.error(
            "Failed to save session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return False


def load_job_session(job_id: str) -> JobState | None:
    """
    Load job state from disk.

    Args:
        job_id: The job ID to load

    Returns:
        JobState if found, None otherwise

    """
    if not _is_safe_job_id(job_id):
        logger.warning("Cannot load session: unsafe job id %s", _sanitize_for_log(job_id))
        return None

    try:
        session_file = resolve_path_within_root(SESSION_DIR, f"{job_id}.json")
        if not session_file.exists():
            return None

        session_data = json.loads(session_file.read_text(encoding="utf-8"))

        # Reconstruct JobState from saved data
        # Need to handle nested models properly
        tdd_data = session_data.pop("tdd_workflow", {})
        deep_data = session_data.pop("deep_understanding", {})

        job = JobState(**session_data)

        # Restore nested models
        if tdd_data:
            job.tdd_workflow = TDDWorkflowState(**tdd_data)
        if deep_data:
            job.deep_understanding = DeepUnderstandingState(**deep_data)

        # Reinitialize hitl_event and hitl_lock (not serialized)
        job.hitl_event = asyncio.Event()
        job.hitl_lock = asyncio.Lock()

        JOBS[job_id] = job
        logger.info("Loaded session for job %s", _sanitize_for_log(job_id))
        return job

    except ValueError as e:
        logger.warning(
            "Cannot load session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return None

    except Exception as e:
        logger.error(
            "Failed to load session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return None


def list_saved_sessions() -> list[str]:
    """
    List all saved session IDs.

    Returns:
        List of job IDs with saved sessions

    """
    try:
        return [f.stem for f in SESSION_DIR.glob("*.json")]
    except Exception:
        return []


def delete_job_session(job_id: str) -> bool:
    """
    Delete a saved session.

    Args:
        job_id: The job ID to delete

    Returns:
        True if deletion succeeded, False otherwise

    """
    if not _is_safe_job_id(job_id):
        logger.warning("Cannot delete session: unsafe job id %s", _sanitize_for_log(job_id))
        return False

    try:
        session_file = resolve_path_within_root(SESSION_DIR, f"{job_id}.json")
        if session_file.exists():
            session_file.unlink()
            return True
        return False
    except ValueError as e:
        logger.warning(
            "Cannot delete session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return False
    except Exception as e:
        logger.error(
            "Failed to delete session for job %s: %s",
            _sanitize_for_log(job_id),
            _sanitize_for_log(e),
        )
        return False


def cleanup_old_sessions(max_age_hours: float = 24.0) -> int:
    """
    Clean up old session files and evict from memory.

    Args:
        max_age_hours: Maximum age in hours before deletion

    Returns:
        Number of sessions cleaned up

    """
    cleaned = 0
    cutoff_time = time.time() - (max_age_hours * 3600)

    # Clean up files
    try:
        for session_file in SESSION_DIR.glob("*.json"):
            if session_file.stat().st_mtime < cutoff_time:
                try:
                    session_file.unlink()
                    cleaned += 1
                except Exception:
                    # Ignore errors when deleting individual files
                    pass
    except Exception:
        # Ignore errors during cleanup (e.g., directory doesn't exist)
        pass

    # Also trigger memory eviction
    JOBS._evict_if_needed()

    if cleaned > 0:
        logger.info(f"Cleaned up {cleaned} old session(s)")

    return cleaned
