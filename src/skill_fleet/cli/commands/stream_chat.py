"""
Real-time streaming CLI chat for skill creation.

This module provides an enhanced chat experience with:
- Real-time streaming output (no polling delays)
- Live thinking/reasoning display
- Arrow key navigation for multi-choice questions
- Immediate response to user input
"""

from __future__ import annotations

import asyncio
import logging
from typing import Any

import httpx
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.spinner import Spinner
from rich.text import Text

from ..ui.prompts import (
    PromptUI,
    choose_many_with_other,
    choose_one_with_other,
    get_default_ui,
)

console = Console()
logger = logging.getLogger(__name__)


class StreamingSkillChat:
    """Real-time streaming chat for skill creation with live updates."""

    def __init__(
        self,
        client: Any,
        console: Console,
        ui: PromptUI | None = None,
        show_thinking: bool = True,
        force_plain_text: bool = False,
    ):
        self.client = client
        self.console = console
        self.ui = ui or get_default_ui(force_plain_text=force_plain_text)
        self.show_thinking = show_thinking
        self.force_plain_text = force_plain_text

    async def create_skill_streaming(
        self, task_description: str, user_id: str = "default"
    ) -> dict[str, Any]:
        """
        Create a skill with real-time streaming updates.

        This provides immediate feedback as the workflow progresses,
        showing thinking, progress, and questions as they arrive.
        """
        # Start the job
        self.console.print(f"[bold cyan]üöÄ Starting:[/bold cyan] {task_description}")

        result = await self.client.create_skill(task_description, user_id)
        job_id = result.get("job_id")

        if not job_id:
            raise ValueError("No job_id returned from API")

        self.console.print(f"[dim]Job ID: {job_id}[/dim]\n")

        # Poll with very short intervals for responsive UI
        return await self._poll_job_fast(job_id)

    async def _poll_job_fast(self, job_id: str) -> dict[str, Any]:
        """
        Fast polling loop with immediate UI updates.

        Uses aggressive polling (100ms) to feel responsive while
        displaying real-time progress and thinking.
        """
        last_status = None
        last_phase = None
        last_message = None
        displayed_thinking = False
        displayed_questions = False

        with Live(
            Panel(
                Spinner("dots", text="[dim]Initializing workflow...[/dim]"),
                border_style="dim",
            ),
            console=self.console,
            refresh_per_second=10,
        ) as live:
            while True:
                try:
                    prompt_data = await self.client.get_hitl_prompt(job_id)
                    status = prompt_data.get("status")

                    # Terminal states
                    if status in {"completed", "failed", "cancelled"}:
                        live.stop()
                        return prompt_data

                    # HITL state - need user input
                    if status in {"pending_hitl", "pending_user_input"}:
                        live.stop()
                        await self._handle_hitl_prompt(job_id, prompt_data)
                        displayed_thinking = False
                        displayed_questions = False
                        # Restart live display after HITL
                        live.start()
                        continue

                    # Running state - show progress
                    current_phase = prompt_data.get("current_phase", "")
                    progress_message = prompt_data.get("progress_message", "")
                    rationale = prompt_data.get("rationale", "")

                    # Show thinking when available
                    if self.show_thinking and rationale and not displayed_thinking:
                        live.stop()
                        self._display_thinking(rationale, current_phase)
                        displayed_thinking = True
                        live.start()

                    # Update display if changed
                    if current_phase != last_phase or progress_message != last_message:
                        last_phase = current_phase
                        last_message = progress_message

                        phase_label = f"[cyan]{current_phase}[/cyan]" if current_phase else ""
                        message = progress_message or f"Processing... ({status})"

                        if phase_label:
                            display_text = f"{phase_label}: {message}"
                        else:
                            display_text = message

                        live.update(
                            Panel(
                                Spinner("dots", text=f"[dim]{display_text}[/dim]"),
                                title="[bold blue]Skill Creation[/bold blue]",
                                border_style="blue",
                            )
                        )

                    # Fast poll interval (100ms for responsiveness)
                    await asyncio.sleep(0.1)

                except Exception as e:
                    live.stop()
                    logger.error(f"Error polling job: {e}")
                    raise

    def _display_thinking(self, rationale: str, phase: str = ""):
        """Display thinking/rationale in a panel."""
        title = f"[dim]üí≠ {phase} Reasoning[/dim]" if phase else "[dim]üí≠ Thinking[/dim]"

        # Check if it's markdown
        has_markdown = any(tok in rationale for tok in ("**", "*", "`", "#", "-"))

        self.console.print(
            Panel(
                Markdown(rationale) if has_markdown else rationale,
                title=title,
                border_style="dim",
            )
        )

    async def _handle_hitl_prompt(self, job_id: str, prompt_data: dict[str, Any]):
        """Handle HITL prompt with proper formatting and navigation."""
        interaction_type = prompt_data.get("type", "unknown")

        if interaction_type == "clarify":
            await self._handle_clarify(job_id, prompt_data)
        elif interaction_type == "confirm":
            await self._handle_confirm(job_id, prompt_data)
        elif interaction_type == "preview":
            await self._handle_preview(job_id, prompt_data)
        elif interaction_type == "validate":
            await self._handle_validate(job_id, prompt_data)
        else:
            # Generic handler
            await self._handle_generic(job_id, prompt_data)

    async def _handle_clarify(self, job_id: str, prompt_data: dict[str, Any]):
        """Handle clarification questions with arrow key navigation."""
        rationale = prompt_data.get("rationale", "")
        if self.show_thinking and rationale:
            self._display_thinking(rationale, "Clarification")

        questions = prompt_data.get("questions", [])
        if not questions:
            # Simple text input
            self.console.print(
                Panel(
                    "Please provide additional information.",
                    title="[bold yellow]ü§î Clarification Needed[/bold yellow]",
                    border_style="yellow",
                )
            )
            answer = await self.ui.ask_text("Your response (or /cancel)", default="")
            if answer.strip().lower() in {"/cancel", "/exit", "/quit"}:
                await self.client.post_hitl_response(job_id, {"action": "cancel"})
            else:
                await self.client.post_hitl_response(job_id, {"answers": {"response": answer}})
            return

        # Multiple questions with proper formatting
        self.console.print(
            Panel(
                f"Please answer the following {len(questions)} question(s).",
                title="[bold yellow]ü§î Clarification Needed[/bold yellow]",
                border_style="yellow",
            )
        )

        answer_blocks = []
        for idx, question in enumerate(questions, 1):
            q_text = self._extract_question_text(question)
            choices, allows_multiple = self._extract_question_options(question)

            # Display question
            has_markdown = any(tok in q_text for tok in ("**", "*", "`", "#", "-"))
            self.console.print(
                Panel(
                    Markdown(q_text) if has_markdown else q_text,
                    title=f"[bold yellow]Question {idx}/{len(questions)}[/bold yellow]",
                    border_style="yellow",
                )
            )

            # Handle choices with arrow key navigation
            if choices:
                if allows_multiple:
                    selected_ids, free_text = await choose_many_with_other(
                        self.ui,
                        "Select option(s) using arrow keys and space",
                        choices,
                    )
                else:
                    selected_ids, free_text = await choose_one_with_other(
                        self.ui,
                        "Select one option using arrow keys",
                        choices,
                    )

                # Format answer
                selected_labels = [label for opt_id, label in choices if opt_id in selected_ids]
                answer = ", ".join(selected_labels)
                if free_text.strip():
                    answer = f"{answer}\nOther: {free_text}" if answer else free_text
            else:
                answer = await self.ui.ask_text("Your answer (or /cancel)", default="")
                if answer.strip().lower() in {"/cancel", "/exit", "/quit"}:
                    await self.client.post_hitl_response(job_id, {"action": "cancel"})
                    return

            answer_blocks.append(f"Q{idx}: {q_text}\nA{idx}: {answer}")

        # Submit all answers
        combined = "\n\n".join(answer_blocks).strip()
        await self.client.post_hitl_response(job_id, {"answers": {"response": combined}})

    async def _handle_confirm(self, job_id: str, prompt_data: dict[str, Any]):
        """Handle confirmation step."""
        rationale = prompt_data.get("rationale", "")
        if self.show_thinking and rationale:
            self._display_thinking(rationale, "Understanding")

        summary = prompt_data.get("summary", "")
        path = prompt_data.get("path", "")
        key_assumptions = prompt_data.get("key_assumptions", [])

        # Build display content
        content_parts = []
        if summary:
            has_md = any(tok in summary for tok in ("**", "*", "`", "#", "-"))
            content_parts.append(Markdown(summary) if has_md else summary)

        if path:
            content_parts.append(Text(f"\nProposed path: {path}", style="dim"))

        if key_assumptions:
            content_parts.append(Text("\nKey assumptions:", style="dim"))
            for assumption in key_assumptions:
                content_parts.append(Text(f"  ‚Ä¢ {assumption}", style="dim"))

        from rich.console import Group

        self.console.print(
            Panel(
                Group(*content_parts)
                if len(content_parts) > 1
                else (content_parts[0] if content_parts else "No summary available."),
                title="[bold cyan]üìã Understanding Summary[/bold cyan]",
                border_style="cyan",
            )
        )

        action = await self.ui.choose_one(
            "How would you like to proceed?",
            [
                ("proceed", "‚úì Proceed with creation"),
                ("revise", "‚úé Request changes"),
                ("cancel", "‚úï Cancel"),
            ],
            default_id="proceed",
        )

        payload: dict[str, Any] = {"action": action}
        if action == "revise":
            payload["feedback"] = await self.ui.ask_text(
                "What would you like to change?", default=""
            )

        await self.client.post_hitl_response(job_id, payload)

    async def _handle_preview(self, job_id: str, prompt_data: dict[str, Any]):
        """Handle content preview step."""
        rationale = prompt_data.get("rationale", "")
        if self.show_thinking and rationale:
            self._display_thinking(rationale, "Generation")

        content = prompt_data.get("content", "")
        highlights = prompt_data.get("highlights", [])

        # Show content preview
        has_md = any(tok in content for tok in ("**", "*", "`", "#", "-")) if content else False
        self.console.print(
            Panel(
                Markdown(content) if has_md else (content or "No preview available."),
                title="[bold blue]üìù Content Preview[/bold blue]",
                border_style="blue",
            )
        )

        if highlights:
            self.console.print("[dim]Highlights:[/dim]")
            for h in highlights:
                self.console.print(f"  ‚Ä¢ {h}")

        action = await self.ui.choose_one(
            "How does this look?",
            [
                ("proceed", "‚úì Looks good, proceed"),
                ("refine", "‚úé Needs refinement"),
                ("cancel", "‚úï Cancel"),
            ],
            default_id="proceed",
        )

        payload: dict[str, Any] = {"action": action}
        if action == "refine":
            payload["feedback"] = await self.ui.ask_text("What needs to be improved?", default="")

        await self.client.post_hitl_response(job_id, payload)

    async def _handle_validate(self, job_id: str, prompt_data: dict[str, Any]):
        """Handle validation step."""
        rationale = prompt_data.get("rationale", "")
        if self.show_thinking and rationale:
            self._display_thinking(rationale, "Validation")

        issues = prompt_data.get("issues", [])
        passed = prompt_data.get("passed", False)

        if issues:
            self.console.print("[yellow]Validation issues found:[/yellow]")
            for issue in issues:
                severity = issue.get("severity", "warning")
                message = issue.get("message", "")
                icon = "‚ùå" if severity == "error" else "‚ö†Ô∏è"
                self.console.print(f"  {icon} {message}")

        action = await self.ui.choose_one(
            "Validation result",
            [
                ("proceed", "‚úì Accept and save"),
                ("refine", "‚úé Fix issues"),
                ("cancel", "‚úï Cancel"),
            ],
            default_id="proceed" if passed else "refine",
        )

        await self.client.post_hitl_response(job_id, {"action": action})

    async def _handle_generic(self, job_id: str, prompt_data: dict[str, Any]):
        """Handle unknown interaction types."""
        message = prompt_data.get("message", "Input required")
        self.console.print(Panel(message, title="[bold]Input Required[/bold]"))

        answer = await self.ui.ask_text("Your response (or /cancel)", default="")
        if answer.strip().lower() in {"/cancel", "/exit", "/quit"}:
            await self.client.post_hitl_response(job_id, {"action": "cancel"})
        else:
            await self.client.post_hitl_response(job_id, {"answers": {"response": answer}})

    def _extract_question_text(self, question: Any) -> str:
        """Extract question text from various formats."""
        if isinstance(question, str):
            return question
        if isinstance(question, dict):
            return str(question.get("text") or question.get("question") or str(question))
        return str(question)

    def _extract_question_options(self, question: Any) -> tuple[list[tuple[str, str]], bool]:
        """Extract options from question format."""
        if not isinstance(question, dict):
            return ([], False)

        raw_options = question.get("options", [])
        if not isinstance(raw_options, list):
            return ([], False)

        choices = []
        for opt in raw_options:
            if isinstance(opt, dict):
                opt_id = str(opt.get("id") or opt.get("value") or "")
                label = str(opt.get("label") or opt.get("text") or opt_id)
                desc = opt.get("description")
                if desc:
                    label = f"{label} ‚Äî {desc}"
                if opt_id:
                    choices.append((opt_id, label))
            else:
                choices.append((str(opt), str(opt)))

        allows_multiple = bool(question.get("allows_multiple", False))
        return (choices, allows_multiple)


async def run_streaming_chat(
    console: Console,
    client: Any,
    task: str | None = None,
    show_thinking: bool = True,
    force_plain_text: bool = False,
):
    """Run the streaming chat interface."""
    chat = StreamingSkillChat(
        client=client,
        console=console,
        show_thinking=show_thinking,
        force_plain_text=force_plain_text,
    )

    console.print(
        Panel.fit(
            "[bold cyan]Skill Fleet ‚Äî Interactive Creator[/bold cyan]\n"
            "Real-time streaming with live thinking display\n"
            "Commands: /help, /exit",
            border_style="cyan",
        )
    )

    def print_help():
        console.print(
            Panel.fit(
                "[bold]Commands[/bold]\n"
                "- /help: show this message\n"
                "- /exit: quit\n\n"
                "[bold]Features[/bold]\n"
                "- Real-time progress updates\n"
                "- Live thinking/reasoning display\n"
                "- Arrow key navigation for choices\n"
                "- Immediate response to questions",
                title="Help",
                border_style="cyan",
            )
        )

    pending_task = task

    while True:
        try:
            if pending_task:
                task_description = pending_task
                pending_task = None
            else:
                task_description = await chat.ui.ask_text(
                    "\nWhat capability would you like to build? (or /help, /exit)"
                )

            if task_description.lower() in {"/exit", "/quit"}:
                return
            if task_description.lower() == "/help":
                print_help()
                continue
            if not task_description.strip():
                continue

            # Run skill creation with streaming
            result = await chat.create_skill_streaming(task_description)

            # Display final result
            status = result.get("status")
            if status == "completed":
                validation_passed = result.get("validation_passed")
                final_path = result.get("final_path") or result.get("saved_path")
                draft_path = result.get("draft_path")

                if validation_passed is False:
                    console.print(
                        "\n[bold yellow]‚ú® Skill Created (validation issues)[/bold yellow]"
                    )
                else:
                    console.print("\n[bold green]‚ú® Skill Created Successfully![/bold green]")

                if final_path:
                    console.print(f"[bold cyan]üìÅ Saved to:[/bold cyan] {final_path}")
                elif draft_path:
                    console.print(f"[bold cyan]üìù Draft saved:[/bold cyan] {draft_path}")

            elif status == "failed":
                console.print(f"\n[red]‚ùå Failed: {result.get('error', 'Unknown error')}[/red]")
            elif status == "cancelled":
                console.print("\n[yellow]‚ö†Ô∏è Cancelled by user[/yellow]")

            # Ask to continue
            again = await chat.ui.choose_one(
                "What would you like to do?",
                [
                    ("create", "Create another skill"),
                    ("exit", "Exit"),
                ],
                default_id="exit",
            )

            if again == "exit":
                return

        except httpx.HTTPStatusError as e:
            console.print(f"[red]HTTP Error: {e.response.status_code}[/red]")
            if e.response.status_code == 404:
                console.print("[yellow]Make sure the server is running:[/yellow]")
                console.print("  uv run skill-fleet serve")
        except Exception as e:
            console.print(f"[red]Error: {type(e).__name__}: {e}[/red]")
            logger.exception("Streaming chat error")
