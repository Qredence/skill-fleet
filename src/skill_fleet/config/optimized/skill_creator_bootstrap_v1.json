{
  "phase1.gather_requirements.gather.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      }
    ],
    "signature": {
      "instructions": "Initial requirements gathering from task description.\n\nExtract basic requirements before detailed analysis:\n- What domain/category?\n- What level (beginner/intermediate/advanced)?\n- What specific topics to cover?\n- Any constraints or preferences?",
      "fields": [
        {
          "prefix": "Task Description:",
          "description": "User's task description (may include clarifications)"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Domain:",
          "description": "Primary domain: 'technical', 'cognitive', 'domain_knowledge', etc."
        },
        {
          "prefix": "Category:",
          "description": "Category within domain: 'programming', 'devops', 'data_science', etc."
        },
        {
          "prefix": "Target Level:",
          "description": "Target level: 'beginner', 'intermediate', 'advanced', 'expert'"
        },
        {
          "prefix": "Topics:",
          "description": "List of specific topics to cover (3-7 items)"
        },
        {
          "prefix": "Constraints:",
          "description": "Any constraints or preferences (e.g., 'focus on Python 3.12+', 'no deprecated patterns')"
        },
        {
          "prefix": "Ambiguities:",
          "description": "Detected ambiguities that need clarification via HITL"
        }
      ]
    },
    "lm": null
  },
  "phase1.analyze_intent.analyze.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      }
    ],
    "signature": {
      "instructions": "Deeply analyze user intent to understand what skill is needed.\n\nThis is one of three parallel analyses in Phase 1. Focus on:\n- WHY is this skill needed?\n- WHAT problem does it solve?\n- WHO is the target user?\n- WHAT value does it provide?\n\nUse chain-of-thought reasoning for thorough analysis.",
      "fields": [
        {
          "prefix": "Task Description:",
          "description": "User's task description with any clarifications"
        },
        {
          "prefix": "User Context:",
          "description": "JSON user context (user_id, existing skills, preferences)"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Task Intent:",
          "description": "Structured intent with: purpose, problem_statement, target_audience, value_proposition"
        },
        {
          "prefix": "Skill Type:",
          "description": "Type of skill: 'how_to', 'reference', 'concept', 'workflow', 'checklist'"
        },
        {
          "prefix": "Scope:",
          "description": "Scope description: what's included and excluded"
        },
        {
          "prefix": "Success Criteria:",
          "description": "How will we know this skill is successful? (3-5 criteria)"
        }
      ]
    },
    "lm": null
  },
  "phase1.find_taxonomy.find_path.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      }
    ],
    "signature": {
      "instructions": "Determine optimal taxonomy placement for this skill.\n\nThis is one of three parallel analyses in Phase 1. Analyze the\ntaxonomy structure and find the best location for this skill.\n\nRules:\n- Prefer deeper paths (more specific is better)\n- Consider existing skills in similar categories\n- Follow taxonomy naming conventions\n- Avoid creating new top-level categories",
      "fields": [
        {
          "prefix": "Task Description:",
          "description": "User's task description"
        },
        {
          "prefix": "Taxonomy Structure:",
          "description": "JSON representation of full taxonomy structure"
        },
        {
          "prefix": "Existing Skills:",
          "description": "List of existing skill paths for reference"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Recommended Path:",
          "description": "Recommended taxonomy path (e.g., 'technical_skills/programming/python/async')"
        },
        {
          "prefix": "Alternative Paths:",
          "description": "2-3 alternative paths if primary has issues"
        },
        {
          "prefix": "Path Rationale:",
          "description": "Why this path is optimal (mention similar skills, category fit, etc.)"
        },
        {
          "prefix": "New Directories:",
          "description": "Any new directories that need to be created (empty if using existing path)"
        },
        {
          "prefix": "Confidence:",
          "description": "Confidence in path selection 0-1. <0.7 means may need user confirmation"
        }
      ]
    },
    "lm": null
  },
  "phase1.analyze_dependencies.analyze": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      }
    ],
    "signature": {
      "instructions": "Analyze skill dependencies and prerequisites.\n\nThis is one of three parallel analyses in Phase 1. Determine:\n- What skills must user know first? (prerequisites)\n- What skills complement this one? (related skills)\n- What skills might conflict? (conflicts)",
      "fields": [
        {
          "prefix": "Task Description:",
          "description": "User's task description"
        },
        {
          "prefix": "Task Intent:",
          "description": "Analyzed task intent (from AnalyzeIntent)"
        },
        {
          "prefix": "Taxonomy Path:",
          "description": "Recommended taxonomy path (from FindTaxonomyPath)"
        },
        {
          "prefix": "Existing Skills:",
          "description": "JSON list of existing skills with metadata"
        },
        {
          "prefix": "Dependency Analysis:",
          "description": "Complete dependency analysis with: required, recommended, conflicts"
        },
        {
          "prefix": "Prerequisite Skills:",
          "description": "Skills user must know first (hard prerequisites)"
        },
        {
          "prefix": "Complementary Skills:",
          "description": "Skills that complement this one (soft recommendations)"
        },
        {
          "prefix": "Missing Prerequisites:",
          "description": "Prerequisites that don't exist yet (need to create first)"
        }
      ]
    },
    "lm": null
  },
  "phase1.synthesize.synthesize.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      }
    ],
    "signature": {
      "instructions": "Synthesize all Phase 1 analyses into a coherent skill creation plan.\n\nCombine results from:\n- Intent analysis\n- Taxonomy path selection\n- Dependency analysis\n\nCreate a unified plan that guides Phase 2 (generation).\n\nThis signature uses dspy.Refine for iterative improvement.",
      "fields": [
        {
          "prefix": "Intent Analysis:",
          "description": "JSON TaskIntent from AnalyzeIntent"
        },
        {
          "prefix": "Taxonomy Analysis:",
          "description": "JSON taxonomy path and rationale from FindTaxonomyPath"
        },
        {
          "prefix": "Dependency Analysis:",
          "description": "JSON DependencyAnalysis from AnalyzeDependencies"
        },
        {
          "prefix": "User Confirmation:",
          "description": "User's confirmation or feedback from HITL checkpoint (may be empty on first pass)"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Skill Metadata:",
          "description": "Complete skill metadata: name, description, taxonomy_path, tags, etc."
        },
        {
          "prefix": "Content Plan:",
          "description": "Outline of skill content: sections, topics, example count, etc."
        },
        {
          "prefix": "Generation Instructions:",
          "description": "Specific instructions for Phase 2 generation (style, tone, depth, etc.)"
        },
        {
          "prefix": "Success Criteria:",
          "description": "How to evaluate if generated content is successful"
        },
        {
          "prefix": "Estimated Length:",
          "description": "Estimated skill length: 'short' (<500 lines), 'medium' (500-1500), 'long' (>1500)"
        }
      ]
    },
    "lm": null
  },
  "phase2.generate_content.generate.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      }
    ],
    "signature": {
      "instructions": "Generate complete SKILL.md content based on the plan.\n\nThis is the main content generation step. Create comprehensive,\nwell-structured skill documentation that follows agentskills.io format.\n\nRequirements:\n- YAML frontmatter at top (name in kebab-case, description)\n- Clear sections with headers\n- Code examples with explanations\n- Best practices and gotchas\n- Usage examples\n- Test cases where applicable\n\nThis signature is used with dspy.BestOfN to generate multiple\ncandidates and select the best one based on quality metrics.",
      "fields": [
        {
          "prefix": "Skill Metadata:",
          "description": "Complete skill metadata from Phase 1 synthesis"
        },
        {
          "prefix": "Content Plan:",
          "description": "Detailed content plan: sections, topics, example count"
        },
        {
          "prefix": "Generation Instructions:",
          "description": "Specific instructions for generation (style, tone, depth)"
        },
        {
          "prefix": "Parent Skills Content:",
          "description": "Content from parent skills for reference and consistency"
        },
        {
          "prefix": "Dependency Summaries:",
          "description": "Summaries of dependency skills (to reference appropriately)"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Skill Content:",
          "description": "Complete SKILL.md content with YAML frontmatter, all sections, examples, and best practices"
        },
        {
          "prefix": "Usage Examples:",
          "description": "3-5 concrete usage examples showing how to use this skill"
        },
        {
          "prefix": "Best Practices:",
          "description": "5-10 best practices and gotchas"
        },
        {
          "prefix": "Test Cases:",
          "description": "Test cases to verify skill understanding (if applicable)"
        },
        {
          "prefix": "Estimated Reading Time:",
          "description": "Estimated reading time in minutes"
        }
      ]
    },
    "lm": null
  },
  "phase2.incorporate_feedback.incorporate.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      }
    ],
    "signature": {
      "instructions": "Incorporate user feedback from HITL preview checkpoint.\n\nUser has reviewed the preview and provided feedback. Make changes\nto the skill content based on their suggestions.\n\nChanges might include:\n- Add missing sections or examples\n- Adjust tone or style\n- Expand or reduce scope\n- Fix inaccuracies\n\nThis signature uses dspy.Refine for iterative improvement.",
      "fields": [
        {
          "prefix": "Current Content:",
          "description": "Current skill content"
        },
        {
          "prefix": "User Feedback:",
          "description": "User's feedback (free-form text or structured change requests)"
        },
        {
          "prefix": "Change Requests:",
          "description": "JSON structured change requests from AnalyzeFeedback"
        },
        {
          "prefix": "Skill Metadata:",
          "description": "Skill metadata for context"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Refined Content:",
          "description": "Refined skill content incorporating all feedback"
        },
        {
          "prefix": "Changes Made:",
          "description": "List of changes made (for user review)"
        },
        {
          "prefix": "Unaddressed Feedback:",
          "description": "Any feedback that couldn't be addressed (with explanation why)"
        },
        {
          "prefix": "Improvement Score:",
          "description": "Self-assessment of improvement 0-1. >0.7 means significant improvement"
        }
      ]
    },
    "lm": null
  },
  "phase3.validator.validate": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      }
    ],
    "signature": {
      "instructions": "Comprehensive validation of generated skill content.\n\nValidate against multiple criteria:\n1. **agentskills.io Compliance**:\n   - YAML frontmatter present and valid\n   - Name in kebab-case\n   - Description present\n\n2. **Content Quality**:\n   - All planned sections present\n   - Sufficient examples (at least 3 per major section)\n   - Code examples are valid and runnable\n   - Best practices included\n\n3. **Structural Integrity**:\n   - Proper markdown formatting\n   - No broken links\n   - Consistent heading levels\n\n4. **Metadata Consistency**:\n   - Metadata matches content\n   - Tags appropriate\n   - Dependencies correctly referenced\n\nReturn a structured validation report with specific issues\nand suggested fixes.",
      "fields": [
        {
          "prefix": "Skill Content:",
          "description": "Complete SKILL.md content to validate"
        },
        {
          "prefix": "Skill Metadata:",
          "description": "Skill metadata"
        },
        {
          "prefix": "Content Plan:",
          "description": "Original content plan from Phase 1 (to verify all planned content is present)"
        },
        {
          "prefix": "Validation Rules:",
          "description": "JSON validation rules and thresholds"
        },
        {
          "prefix": "Validation Report:",
          "description": "Complete validation report with: passed, checks, issues, warnings"
        },
        {
          "prefix": "Critical Issues:",
          "description": "Issues that MUST be fixed (blocks skill acceptance)"
        },
        {
          "prefix": "Warnings:",
          "description": "Issues that SHOULD be fixed but not blocking"
        },
        {
          "prefix": "Suggestions:",
          "description": "Optional suggestions for improvement"
        },
        {
          "prefix": "Overall Score:",
          "description": "Overall quality score 0-1. >0.8 means high quality"
        }
      ]
    },
    "lm": null
  },
  "phase3.refiner.refine.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      }
    ],
    "signature": {
      "instructions": "Refine skill content based on validation issues and user feedback.\n\nThis signature is used with dspy.Refine for iterative improvement.\nEach iteration should:\n1. Address highest priority issues first\n2. Maintain consistency with existing content\n3. Preserve what's working well\n4. Re-validate after changes\n\nContinue refining until:\n- All critical issues resolved\n- User is satisfied (via HITL)\n- Max iterations reached",
      "fields": [
        {
          "prefix": "Current Content:",
          "description": "Current skill content"
        },
        {
          "prefix": "Validation Issues:",
          "description": "JSON list of validation issues to address"
        },
        {
          "prefix": "User Feedback:",
          "description": "User's feedback on what to change (may be empty for auto-fix)"
        },
        {
          "prefix": "Fix Strategies:",
          "description": "JSON fix strategies from AnalyzeValidationIssues"
        },
        {
          "prefix": "Iteration Number:",
          "description": "Current iteration number (1, 2, 3, ...)"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Refined Content:",
          "description": "Refined skill content with issues addressed"
        },
        {
          "prefix": "Issues Resolved:",
          "description": "List of issue IDs that were resolved in this iteration"
        },
        {
          "prefix": "Issues Remaining:",
          "description": "List of issue IDs still remaining (empty if all resolved)"
        },
        {
          "prefix": "Changes Summary:",
          "description": "Summary of changes made in this iteration (for user review)"
        },
        {
          "prefix": "Ready For Acceptance:",
          "description": "True if all critical issues resolved and ready for user acceptance"
        }
      ]
    },
    "lm": null
  },
  "phase3.quality_assessor.assess.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      }
    ],
    "signature": {
      "instructions": "Assess overall quality of skill content beyond validation.\n\nWhile ValidateSkill checks compliance and structure, this signature\nevaluates content quality:\n- Are examples clear and helpful?\n- Is the writing engaging?\n- Are explanations thorough?\n- Is the skill actually useful?\n\nProvides qualitative feedback for refinement.",
      "fields": [
        {
          "prefix": "Skill Content:",
          "description": "Complete skill content"
        },
        {
          "prefix": "Skill Metadata:",
          "description": "Skill metadata"
        },
        {
          "prefix": "Target Level:",
          "description": "Target level: beginner/intermediate/advanced"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Quality Score:",
          "description": "Overall quality score 0-1. >0.8 is excellent, 0.6-0.8 is good, <0.6 needs improvement"
        },
        {
          "prefix": "Strengths:",
          "description": "What's good about this skill (3-5 points)"
        },
        {
          "prefix": "Weaknesses:",
          "description": "What could be improved (3-5 points)"
        },
        {
          "prefix": "Recommendations:",
          "description": "Specific recommendations for improvement"
        },
        {
          "prefix": "Audience Alignment:",
          "description": "How well content matches target level 0-1. >0.8 means well-aligned"
        }
      ]
    },
    "lm": null
  },
  "hitl_strategy.determine_strategy.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      }
    ],
    "signature": {
      "instructions": "Determine optimal HITL strategy for a given task.\n\nAnalyze the task and decide:\n- Which HITL checkpoints are needed\n- How many questions to ask\n- Whether auto-approve is appropriate",
      "fields": [
        {
          "prefix": "Task Description:",
          "description": "User's task description"
        },
        {
          "prefix": "Task Complexity:",
          "description": "Complexity assessment: 'simple', 'moderate', 'complex'"
        },
        {
          "prefix": "User Preferences:",
          "description": "JSON of user preferences (e.g., prefer_auto_approve, verbose_feedback)"
        },
        {
          "prefix": "Reasoning:",
          "description": "Reasoning for this strategy"
        },
        {
          "prefix": "Strategy:",
          "description": "Recommended strategy: 'minimal' (2 checkpoints), 'standard' (4 checkpoints), 'thorough' (6 checkpoints)"
        },
        {
          "prefix": "Checkpoints:",
          "description": "List of checkpoint names to enable (e.g., ['phase1_clarify', 'phase1_confirm', ...])"
        }
      ]
    },
    "lm": null
  },
  "readiness.assess": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      }
    ],
    "signature": {
      "instructions": "Assess if we're ready to proceed to next phase.\n\nEvaluate whether we have enough information to proceed,\nor if more HITL interaction is needed.",
      "fields": [
        {
          "prefix": "Phase:",
          "description": "Current phase: 'understanding', 'generation', 'validation'"
        },
        {
          "prefix": "Collected Info:",
          "description": "JSON of information collected so far"
        },
        {
          "prefix": "Min Requirements:",
          "description": "JSON of minimum requirements needed to proceed"
        },
        {
          "prefix": "Ready:",
          "description": "True if ready to proceed to next phase"
        },
        {
          "prefix": "Readiness Score:",
          "description": "Readiness score 0-1. >0.8 means ready"
        },
        {
          "prefix": "Missing Info:",
          "description": "List of missing information needed (empty if ready)"
        },
        {
          "prefix": "Next Questions:",
          "description": "Suggested next questions to ask user (if not ready)"
        }
      ]
    },
    "lm": null
  },
  "confirm_understanding.summarize": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      }
    ],
    "signature": {
      "instructions": "Summarize understanding of user intent for confirmation.\n\nCreate a clear, concise summary of what we understood from the task\nand user's clarifying answers. This is shown to user for confirmation\nbefore proceeding to expensive generation phase.\n\nFormat as bullet points for easy scanning.",
      "fields": [
        {
          "prefix": "Task Description:",
          "description": "Original task description"
        },
        {
          "prefix": "User Clarifications:",
          "description": "JSON string of user's answers to clarifying questions"
        },
        {
          "prefix": "Intent Analysis:",
          "description": "Analyzed intent from Phase 1 parallel analysis"
        },
        {
          "prefix": "Taxonomy Path:",
          "description": "Determined taxonomy path (e.g., technical_skills/programming/python)"
        },
        {
          "prefix": "Dependencies:",
          "description": "List of skill dependencies"
        },
        {
          "prefix": "Summary:",
          "description": "Concise bullet-point summary of understanding (3-5 bullets)"
        },
        {
          "prefix": "Key Assumptions:",
          "description": "Key assumptions being made (user should verify these)"
        },
        {
          "prefix": "Confidence:",
          "description": "Confidence score 0-1. >0.8 means high confidence, proceed. <0.8 means may need more clarification"
        }
      ]
    },
    "lm": null
  },
  "preview_generator.generate_preview": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      }
    ],
    "signature": {
      "instructions": "Generate a preview of skill content for user review.\n\nCreate a concise preview showing:\n- Skill structure (sections/headings)\n- Key points covered in each section\n- Example count\n- Estimated length\n\nThis helps user verify scope and style before full generation.",
      "fields": [
        {
          "prefix": "Skill Content:",
          "description": "Full generated skill content (SKILL.md)"
        },
        {
          "prefix": "Metadata:",
          "description": "JSON skill metadata (name, description, capabilities)"
        },
        {
          "prefix": "Preview:",
          "description": "Concise preview with: 1) Table of contents, 2) Key points per section, 3) Stats (examples, length)"
        },
        {
          "prefix": "Highlights:",
          "description": "3-5 highlights of what makes this skill valuable"
        },
        {
          "prefix": "Potential Issues:",
          "description": "Potential issues user might want to address (e.g., 'No error handling examples')"
        }
      ]
    },
    "lm": null
  },
  "feedback_analyzer.analyze.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      }
    ],
    "signature": {
      "instructions": "Analyze user feedback and determine what changes to make.\n\nParse user's free-form feedback and convert it into structured\nchange requests that can be used to refine the skill content.",
      "fields": [
        {
          "prefix": "User Feedback:",
          "description": "User's feedback on the preview (free-form text)"
        },
        {
          "prefix": "Current Content:",
          "description": "Current skill content"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Change Requests:",
          "description": "Structured change requests: [{type: 'add/remove/modify', section: '...', details: '...'}]"
        },
        {
          "prefix": "Scope Change:",
          "description": "True if feedback requires major scope change (may need to restart)"
        },
        {
          "prefix": "Estimated Effort:",
          "description": "Estimated effort: 'minor' (quick fix), 'moderate' (refinement), 'major' (regeneration)"
        }
      ]
    },
    "lm": null
  },
  "validation_formatter.format_results": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      }
    ],
    "signature": {
      "instructions": "Format validation results for human-readable display.\n\nConvert technical validation output into clear, actionable\nfeedback for the user. Group issues by severity and provide\nsuggested fixes.",
      "fields": [
        {
          "prefix": "Validation Report:",
          "description": "JSON validation report with checks, failures, warnings"
        },
        {
          "prefix": "Skill Content:",
          "description": "The skill content that was validated"
        },
        {
          "prefix": "Formatted Report:",
          "description": "Human-readable report with: 1) Summary (pass/fail), 2) Issues by severity, 3) Suggested fixes"
        },
        {
          "prefix": "Critical Issues:",
          "description": "Critical issues that MUST be fixed before acceptance"
        },
        {
          "prefix": "Warnings:",
          "description": "Warnings that SHOULD be addressed but not blocking"
        },
        {
          "prefix": "Auto Fixable:",
          "description": "True if all issues can be auto-fixed without user input"
        }
      ]
    },
    "lm": null
  },
  "refinement_planner.plan_refinement.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/asynchronous_programming",
        "skill_content": "---\nname: python-asynchronous-programming\ndescription: Implementation and management of non-blocking code using asyncio, event\n  loops, and concurrent execution patterns in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/asynchronous_programming\n  version: 1.0.0\n---\n\n# Python Asynchronous Programming (asyncio)\n\n## Overview\nAsynchronous programming in Python, powered by the `asyncio` library, enables high-performance concurrent execution using a single-threaded event loop. This skill covers the transition from traditional blocking code to non-blocking, cooperative multitasking, allowing for thousands of simultaneous connections without the overhead of heavy threading.\n\n## Core Concepts\n- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n- **Event Loop**: The central scheduler that manages and executes asynchronous tasks.\n- **Awaitables**: Objects that can be used in an `await` expression (Coroutines, Tasks, Futures).\n- **Non-blocking I/O**: Performing input/output operations without stalling the execution of other tasks.\n\n## Key Components\n- **Task Management**: Using `asyncio.create_task` for concurrent execution and `asyncio.gather` for aggregating results.\n- **Error Handling**: Managing `asyncio.CancelledError` and timeouts to ensure system resilience.\n- **Synchronization**: Using async-aware `Locks`, `Semaphores`, and `Queues` to coordinate state between coroutines.\n- **Blocking Interop**: Offloading CPU-bound or legacy blocking I/O to threads or processes via `run_in_executor`."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/languages/python/decorators",
        "skill_content": "---\nname: python-decorators\ndescription: Ability to design, implement, and apply higher-order functions to extend\n  or modify the behavior of functions and classes in Python.\nmetadata:\n  skill_id: technical_skills/programming/languages/python/decorators\n  version: 1.0.0\n---\n\n# Python Decorators: Functional and Metaprogramming Patterns\n\n## Overview\nPython decorators are a powerful form of metaprogramming used to modify or enhance the behavior of functions or classes without permanently modifying their source code. They rely on Python's first-class function support and closure mechanics.\n\n## Core Concepts\n- **Closures**: Functions that \"remember\" the environment in which they were created.\n- **First-Class Citizens**: The ability to pass functions as arguments, return them from other functions, and assign them to variables.\n- **Syntactic Sugar**: The `@decorator` syntax is equivalent to `func = decorator(func)`.\n\n## Modules\n\n### 1. Basic Function Decorators\nFocuses on the standard wrapper pattern.\n- **Simple Logger**: Intercepting calls to log arguments and return values.\n- **Metadata Preservation**: Critical use of `functools.wraps` to prevent losing the original function's identity (`__name__`, `__doc__`).\n\n### 2. Parameterized Decorators (Decorator Factories)\nMoving beyond simple wrappers to \"factories\" that return decorators.\n- Implementation of the triple-nested function structure: `outer_params(actual_decorator(wrapped_func))`.\n- Use cases: `@retry(times=3)`, `@access_level(\"admin\")`.\n\n### 3. Class-Based Decorators\nUtilizing the `__call__` dunder method to treat an object instance as a decorator.\n- Maintaining state within instance attributes rather than closures.\n\n### 4. Stateful & Memoization\nAdvanced patterns for performance and tracking.\n- **Call Counting**: Monitoring execution frequency.\n- **Memoization**: Caching expensive computation results based on input arguments (e.g., implementing a custom LRU cache).\n\n### 5. Composition and Order\nUnderstanding the \"Onion\" model of execution.\n- Order of application: Bottom-to-top.\n- Order of execution: Top-to-bottom."
      },
      {
        "task_description": "Create a skill based on: technical_skills/programming/web_frameworks/python/fastapi",
        "skill_content": "---\nname: fastapi-production-patterns\ndescription: Use when building FastAPI apps with async database operations, complex dependency injection, partial update endpoints, async testing, or converting Python utilities to API endpoints\nlicense: MIT\ncompatibility: Requires Python 3.8+, FastAPI 0.128.0+, SQLAlchemy 2.0+\nmetadata:\n  skill_id: technical/programming/web-frameworks/python/fastapi\n  version: 1.0.0\n  type: technical\n  weight: medium\n  load_priority: task_specific\n---\n\n# FastAPI Production Patterns\n\n## Overview\nProven patterns for building production-ready FastAPI applications that avoid common silent failures: connection pool exhaustion, blocking async operations, partial update bugs, and test isolation issues.\n\n**Core principle:** Code that works in development but fails under load is NOT production-ready. Database engines MUST be created in lifespan, connections MUST be disposed on shutdown, and async operations MUST NOT block.\n\n## When to Use\n**When to use:**\n\n```dot\ndigraph when_to_use {\n    \"Building FastAPI app?\" [shape=diamond];\n    \"Async DB operations?\" [shape=diamond];\n    \"Need to test async endpoints?\" [shape=diamond];\n    \"Converting Python utilities to APIs?\" [shape=diamond];\n    \"PATCH with partial updates?\" [shape=diamond];\n    \"Use this skill\" [shape=box];\n\n    \"Building FastAPI app?\" -> \"Async DB operations?\" [label=\"yes\"];\n    \"Async DB operations?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Need to test async endpoints?\" [label=\"yes\"];\n    \"Need to test async endpoints?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"Converting Python utilities to APIs?\" [label=\"yes\"];\n    \"Converting Python utilities to APIs?\" -> \"Use this skill\" [label=\"yes\"];\n    \"Building FastAPI app?\" -> \"PATCH with partial updates?\" [label=\"yes\"];\n    \"PATCH with partial updates?\" -> \"Use this skill\" [label=\"yes\"];\n}\n```\n\n**When NOT to use:**\n- Simple synchronous APIs without database operations\n- Learning basic FastAPI routing (use official docs instead)\n- Trivial CRUD with no production requirements\n\n## Quick Reference\n\n| Problem                                   | Solution                                                   | Keywords                                               |\n|-------------------------------------------|------------------------------------------------------------|--------------------------------------------------------|\n| DB connections not closing on shutdown    | Use `lifespan` context manager with `engine.dispose()`     | connection leak, too many connections, pool exhaustion |\n| Pool exhaustion under load                | Set `pool_size`, `max_overflow`, create engine in lifespan | connection timeout, concurrent requests, workers       |\n| Tests pass isolation but fail in parallel | Use async fixtures with proper isolation                   | flaky tests, test bleeding, async test                 |\n| PATCH partial updates not validating      | Use `Optional` fields with `exclude_unset=True`            | partial updates, None overwriting, PATCH               |\n| Converting sync utilities to async        | Replace blocking libraries with async equivalents          | blocking, async conversion, requests to httpx          |\n| Long operations timeout HTTP              | Use `BackgroundTasks` or Celery                            | timeout, long running, background                      |\n\n## Core Patterns\n\n### 1. Database Lifecycle Management\n\n**The problem:** Engines created at import time never close connections. Deprecated `@app.on_event` handlers miss proper cleanup. Missing pool parameters cause exhaustion under load.\n\n**❌ Common but broken (baseline failure):**\n```python\n# database.py - Created at import time!\nengine = create_async_engine(DATABASE_URL)\n\n# main.py - Deprecated pattern\n@app.on_event(\"startup\")\nasync def startup():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n# NO shutdown handler - connections leak forever!\n```\n\n**✅ Production pattern:**\n```python\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup - create engine HERE, not at import\n    engine = create_async_engine(\n        DATABASE_URL,\n        pool_size=10,           # Critical for multi-worker deployments\n        max_overflow=20,        # Allow bursting above pool_size\n        pool_recycle=3600,      # Recycle connections after 1 hour\n    )\n    app.state.db_engine = engine\n\n    # Create tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    yield\n\n    # Shutdown - CRITICAL: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n\nasync def get_db() -> AsyncSession:\n    async with AsyncSession(app.state.db_engine) as session:\n        yield session\n```\n\n**Key insight:** Engine creation in lifespan + dispose in shutdown = no connection leaks. Pool parameters prevent exhaustion when multiple workers start simultaneously.\n\n### 2. Pydantic Partial Updates\n\n**The problem:** PATCH endpoints should only update provided fields, but naive implementations overwrite everything with `None`.\n\n**❌ Common mistake (None overwrites):**\n```python\n@app.patch(\"/users/{user_id}\")\nasync def update_user(user_id: int, update: UserUpdate, db: AsyncSession = Depends(get_db)):\n    user = await db.get(User, user_id)\n    # ❌ This sets unprovided fields to None!\n    user.name = update.name  # None if not provided\n    user.email = update.email  # None if not provided\n    await db.commit()\n    return user\n```\n\n**✅ Production pattern:**\n```python\nfrom pydantic import BaseModel, Optional\nfrom typing import Optional\n\nclass UserUpdate(BaseModel):\n    name: Optional[str] = None\n    email: Optional[str] = None\n    age: Optional[int] = None\n\n@app.patch(\"/users/{user_id}\")\nasync def update_user(\n    user_id: int,\n    update: UserUpdate,\n    db: AsyncSession = Depends(get_db)\n):\n    user = await db.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # CRITICAL: Only update provided fields\n    update_data = update.model_dump(exclude_unset=True)\n    for field, value in update_data.items():\n        setattr(user, field, value)\n\n    await db.commit()\n    await db.refresh(user)\n    return user\n```\n\n**Key insight:** `exclude_unset=True` only includes fields actually provided in the request, preventing `None` overwrites.\n\n### 3. Converting Python Functions to Endpoints\n\n**The problem:** Existing Python code needs API exposure. Common mistakes: blocking operations, missing validation, no error handling, wrong return types.\n\n**Transformation steps:**\n1. Add Pydantic models for request/response\n2. Make functions async if they do I/O\n3. Replace exceptions with `HTTPException`\n4. Add `response_model` for validation\n5. Use `Depends` for shared resources\n\n**Before - Blocking utility:**\n```python\ndef process_payment(user_id: int, amount: float, card: dict) -> dict:\n    result = db.execute(f\"SELECT * FROM users WHERE id = {user_id}\")  # Blocking!\n    if result['balance'] < amount:\n        raise ValueError(\"Insufficient funds\")  # 500 error!\n    return {\"status\": \"success\"}\n\n# Naive wrapper\n@app.post(\"/payment\")\ndef payment_endpoint(user_id: int, amount: float, card: dict):\n    return process_payment(user_id, amount, card)  # No validation!\n```\n\n**After - Production endpoint:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom fastapi import HTTPException\n\nclass CreditCard(BaseModel):\n    number: str = Field(..., min_length=13, max_length=19)\n    expiry: str\n    cvv: str = Field(..., min_length=3, max_length=4)\n\n    @validator('number')\n    def luhn_check(cls, v):\n        if not luhn_valid(v):\n            raise ValueError('Invalid card number')\n        return v\n\nclass PaymentRequest(BaseModel):\n    user_id: int\n    amount: float = Field(..., gt=0)  # Must be positive\n    card: CreditCard\n\nclass PaymentResponse(BaseModel):\n    status: str\n    transaction_id: str\n\n@app.post(\"/payment\", response_model=PaymentResponse)\nasync def payment_endpoint(\n    request: PaymentRequest,\n    db: AsyncSession = Depends(get_db)\n):\n    # Async database call\n    user = await db.get(User, request.user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    if user.balance < request.amount:\n        raise HTTPException(status_code=400, detail=\"Insufficient funds\")\n\n    transaction_id = await process_payment_async(request, db)\n    return PaymentResponse(status=\"success\", transaction_id=transaction_id)\n```\n\n### 4. Sync to Async Conversion\n\n**Sync → Async library mapping:**\n\n| Sync Library   | Async Replacement           |\n|----------------|-----------------------------|\n| `requests`     | `httpx.AsyncClient`         |\n| `sqlalchemy`   | `sqlalchemy.ext.asyncio`    |\n| `time.sleep()` | `asyncio.sleep()`           |\n| `open()`       | `aiofiles`                  |\n| `subprocess`   | `asyncio.create_subprocess` |\n| `redis`        | `aioredis`                  |\n\n**Pattern:**\n```python\n# Before - blocking\ndef get_user_data(user_id: int) -> dict:\n    user = db.session.query(User).filter(User.id == user_id).first()\n    response = requests.get(f\"https://api.external.com/user/{user_id}\")\n    return {\"user\": user, \"external\": response.json()}\n\n# After - async\nasync def get_user_data(user_id: int, db: AsyncSession) -> dict:\n    result = await db.execute(select(User).where(User.id == user_id))\n    user = result.scalar_one_or_none()\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.external.com/user/{user_id}\")\n\n    return {\"user\": user, \"external\": response.json()}\n```\n\n### 5. Dependency Injection Patterns\n\n**Caching dependencies:**\n```python\nfrom functools import lru_cache\n\n@lru_cache()\ndef get_settings():\n    return Settings()  # Singleton, created once\n\n@lru_cache()\ndef get_redis_client():\n    return redis.Redis(host=settings.REDIS_HOST)\n```\n\n**Testing with overrides:**\n```python\nfrom fastapi.testclient import TestClient\n\ndef test_update_user():\n    async def override_get_db():\n        return test_session  # Use test DB\n\n    app.dependency_overrides[get_db] = override_get_db\n    try:\n        response = client.patch(\"/users/1\", json={\"name\": \"Test\"})\n    finally:\n        app.dependency_overrides.clear()  # Always clean up\n```\n\n**Yield dependencies for cleanup:**\n```python\nasync def get_db():\n    async with AsyncSession(engine) as session:\n        yield session\n        # Automatic cleanup after response\n```\n\n### 6. Async Testing\n\n**The problem:** Using sync `TestClient` with async endpoints causes hangs. Tests pass in isolation but fail together due to database bleeding.\n\n**✅ Production pattern:**\n```python\nimport pytest\nfrom httpx import AsyncClient\n\n@pytest.fixture\nasync def async_client():\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        yield ac\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL)\n    async with AsyncSession(engine) as session:\n        yield session\n        await session.rollback()  # Clean up after test\n\n@pytest.mark.asyncio\nasync def test_create_user(async_client: AsyncClient):\n    response = await async_client.post(\"/users\", json={\n        \"name\": \"Alice\",\n        \"email\": \"alice@example.com\"\n    })\n    assert response.status_code == 200\n```\n\n### 7. File Upload Handling\n\n**Stream, don't load entirely:**\n```python\nfrom fastapi import UploadFile\nimport pandas as pd\n\n@app.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile):\n    # Stream the file - don't load entirely into memory\n    df = pd.read_csv(file.file)\n    results = process_data_frame(df)\n    return {\"uploaded\": len(results), \"data\": results}\n```\n\n### 8. Background Tasks\n\n**For operations longer than HTTP timeout:**\n```python\nfrom fastapi import BackgroundTasks\n\ndef long_running_task(task_id: str):\n    result = process_heavy_computation()\n    mark_task_complete(task_id, result)\n\n@app.post(\"/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    task_id = generate_task_id()\n    background_tasks.add_task(long_running_task, task_id)\n    return {\"task_id\": task_id, \"status\": \"processing\"}\n\n# For production: Use Celery for retries and distributed execution\n```\n\n### 9. FastAPI CLI (New in 0.128.0)\n\n**The problem:** Running FastAPI apps required manual uvicorn commands. Developers had to remember different commands for development vs production.\n\n**❌ Old approach (still works but not recommended):**\n```bash\n# Development with auto-reload\nuvicorn main:app --reload\n\n# Production\nuvicorn main:app --host 0.0.0.0 --port 8000\n```\n\n**✅ Modern approach with FastAPI CLI:**\n```bash\n# Installation (includes CLI + all standard dependencies)\npip install \"fastapi[standard]\"\n\n# Development with auto-reload\nfastapi dev main.py\n\n# Production\nfastapi run main.py --workers 4\n```\n\n**CLI Command Reference:**\n\n| Command                           | Purpose            | Auto-reload |\n|-----------------------------------|--------------------|-------------|\n| `fastapi dev main.py`             | Development server | ✅ Yes       |\n| `fastapi run main.py`             | Production server  | ❌ No        |\n| `fastapi dev main.py --port 8080` | Custom port        | ✅ Yes       |\n| `fastapi run main.py --workers 4` | Multiple workers   | ❌ No        |\n\n**Key benefits:**\n- Unified CLI for development and production\n- Auto-reload by default in dev mode\n- Includes all standard dependencies (uvicorn, httpx, jinja2, python-multipart)\n- Better error messages and output formatting\n\n> **For comprehensive CLI documentation**, see the [FastAPI CLI capability](capabilities/fastapi-cli.md)\n\n**Installation with all standard dependencies:**\n```bash\npip install \"fastapi[standard]\"\n```\n\nThis installs:\n- `fastapi` - Core framework\n- `uvicorn[standard]` - ASGI server with high-performance dependencies\n- `fastapi-cli[standard]` - CLI tool (includes deployment tools)\n- `httpx` - For testing\n- `jinja2` - For templates\n- `python-multipart` - For form data\n\n**Optional: FastAPI Cloud deployment:**\n```bash\nfastapi login\nfastapi deploy\n```\n\n> **Note:** FastAPI Cloud is optional. You can deploy to any cloud provider (AWS, GCP, Azure, Railway, etc.) using traditional deployment methods.\n\n## Common Mistakes\n\n| Mistake                             | Why It's Wrong                                    | Fix                                            |\n|-------------------------------------|---------------------------------------------------|------------------------------------------------|\n| Creating DB engine at import time   | Connections never close, workers leak connections | Create in `lifespan`, dispose in shutdown      |\n| Using `requests` in async endpoints | Blocks entire event loop                          | Use `httpx.AsyncClient`                        |\n| Forgetting `exclude_unset=True`     | Optional fields become `None` and overwrite data  | Use `exclude_unset=True` for PATCH             |\n| Sync fixtures with async tests      | Tests hang or fail mysteriously                   | Use `@pytest.mark.asyncio` with async fixtures |\n| Global state for dependencies       | Can't test, hard to manage lifecycle              | Use `Depends()` with yield                     |\n| Not setting `pool_recycle`          | Database closes idle connections, causing errors  | Set `pool_recycle=3600` or similar             |\n| Using `run_in_executor` as band-aid | Still blocks threads, doesn't scale               | Proper async conversion                        |\n| Missing `max_overflow` parameter    | Pool can't burst under load, requests queue       | Set `max_overflow=20` or similar               |\n\n## Real-World Impact\n\n- **Connection pool exhaustion** fixed with proper lifecycle management → 50 concurrent requests without errors\n- **Test execution time** reduced 80% with proper async fixtures\n- **API response validation** caught 15% of frontend bugs before deployment\n- **Memory usage** reduced 60% by streaming file uploads instead of loading into memory\n- **\"Too many connections\" DB errors** eliminated with proper shutdown handling\n\n## Red Flags - STOP and Reconsider\n\n- Code that \"works locally\" but you haven't load tested\n- Creating database connections at module level\n- Using `@app.on_event(\"startup\")` (deprecated)\n- Missing shutdown handlers\n- No pool parameters on engine creation\n- Using `requests` library in async functions\n- Tests that only pass when run individually\n- `run_in_executor` as primary async strategy\n\n**All of these mean: Revisit your architecture. Production will break.**\n"
      }
    ],
    "signature": {
      "instructions": "Generate a refinement plan based on validation issues and user feedback.\n\nCreate a structured plan for how to refine the skill to address\nvalidation failures and incorporate user's feedback.",
      "fields": [
        {
          "prefix": "Validation Issues:",
          "description": "JSON list of validation issues"
        },
        {
          "prefix": "User Feedback:",
          "description": "User's feedback on how to address issues (may be empty for auto-fix)"
        },
        {
          "prefix": "Current Skill:",
          "description": "Current skill content"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Refinement Plan:",
          "description": "Step-by-step plan for refining the skill"
        },
        {
          "prefix": "Changes:",
          "description": "Specific changes to make: [{section: '...', change_type: 'add/remove/modify', details: '...'}]"
        },
        {
          "prefix": "Estimated Iterations:",
          "description": "Estimated number of refinement iterations needed (1-3)"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.13",
      "dspy": "3.1.0",
      "cloudpickle": "3.1"
    }
  }
}
